{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING PROJECT I\n",
    "\n",
    "#### TEAM MEMBERS: ASLI YORUSUN - ERDEM BOCUGOZ - SERIF SONER SERBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from test import *\n",
    "from data_cleaning import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction,data,id_ = load_csv_data(\"train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.where(data==-999, np.nan, data)\n",
    "#null_counts = data.isnull().sum()\n",
    "#print(null_counts)\n",
    "\n",
    "data = np.delete(data, [4,5,6,12,26,27,28], 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(data)\n",
    "y, tx = build_model_data(prediction,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST LEAST SQUARES AND RIDGE REGRESSION\n",
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    data = np.loadtxt(\"dataEx3.csv\", delimiter=\",\", skiprows=1, unpack=True)\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    return x, y\n",
    "\n",
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5000000000000001\n",
      "Gradient Descent(1/999): loss=0.49823370920696813\n",
      "Gradient Descent(2/999): loss=0.49665918418101845\n",
      "Gradient Descent(3/999): loss=0.4952556012695155\n",
      "Gradient Descent(4/999): loss=0.4940043980394159\n",
      "Gradient Descent(5/999): loss=0.4928890277336968\n",
      "Gradient Descent(6/999): loss=0.49189474039111464\n",
      "Gradient Descent(7/999): loss=0.49100838773395034\n",
      "Gradient Descent(8/999): loss=0.490218249242799\n",
      "Gradient Descent(9/999): loss=0.48951387711772365\n",
      "Gradient Descent(10/999): loss=0.48888595807492313\n",
      "Gradient Descent(11/999): loss=0.4883261901507599\n",
      "Gradient Descent(12/999): loss=0.4878271728835165\n",
      "Gradient Descent(13/999): loss=0.4873823094202044\n",
      "Gradient Descent(14/999): loss=0.4869857192534963\n",
      "Gradient Descent(15/999): loss=0.486632160434469\n",
      "Gradient Descent(16/999): loss=0.4863169602321843\n",
      "Gradient Descent(17/999): loss=0.4860359533228785\n",
      "Gradient Descent(18/999): loss=0.48578542669112584\n",
      "Gradient Descent(19/999): loss=0.48556207051412775\n",
      "Gradient Descent(20/999): loss=0.48536293437942996\n",
      "Gradient Descent(21/999): loss=0.4851853882569128\n",
      "Gradient Descent(22/999): loss=0.48502708770879455\n",
      "Gradient Descent(23/999): loss=0.48488594287744435\n",
      "Gradient Descent(24/999): loss=0.48476009084078\n",
      "Gradient Descent(25/999): loss=0.4846478709695625\n",
      "Gradient Descent(26/999): loss=0.48454780296061983\n",
      "Gradient Descent(27/999): loss=0.48445856725541925\n",
      "Gradient Descent(28/999): loss=0.48437898758496795\n",
      "Gradient Descent(29/999): loss=0.48430801541014545\n",
      "Gradient Descent(30/999): loss=0.4842447160516449\n",
      "Gradient Descent(31/999): loss=0.484188256326049\n",
      "Gradient Descent(32/999): loss=0.4841378935244944\n",
      "Gradient Descent(33/999): loss=0.4840929655881289\n",
      "Gradient Descent(34/999): loss=0.4840528823504087\n",
      "Gradient Descent(35/999): loss=0.48401711773038414\n",
      "Gradient Descent(36/999): loss=0.4839852027737089\n",
      "Gradient Descent(37/999): loss=0.48395671944932\n",
      "Gradient Descent(38/999): loss=0.48393129511972877\n",
      "Gradient Descent(39/999): loss=0.48390859761177846\n",
      "Gradient Descent(40/999): loss=0.4838883308226631\n",
      "Gradient Descent(41/999): loss=0.48387023080308345\n",
      "Gradient Descent(42/999): loss=0.483854062265731\n",
      "Gradient Descent(43/999): loss=0.4838396154729102\n",
      "Gradient Descent(44/999): loss=0.4838267034621318\n",
      "Gradient Descent(45/999): loss=0.4838151595729765\n",
      "Gradient Descent(46/999): loss=0.4838048352425124\n",
      "Gradient Descent(47/999): loss=0.4837955980401084\n",
      "Gradient Descent(48/999): loss=0.4837873299156425\n",
      "Gradient Descent(49/999): loss=0.4837799256379375\n",
      "Gradient Descent(50/999): loss=0.48377329140276426\n",
      "Gradient Descent(51/999): loss=0.4837673435920011\n",
      "Gradient Descent(52/999): loss=0.48376200766753436\n",
      "Gradient Descent(53/999): loss=0.48375721718526954\n",
      "Gradient Descent(54/999): loss=0.48375291291621\n",
      "Gradient Descent(55/999): loss=0.48374904206297625\n",
      "Gradient Descent(56/999): loss=0.4837455575614035\n",
      "Gradient Descent(57/999): loss=0.4837424174579777\n",
      "Gradient Descent(58/999): loss=0.4837395843548746\n",
      "Gradient Descent(59/999): loss=0.4837370249152624\n",
      "Gradient Descent(60/999): loss=0.48373470942232244\n",
      "Gradient Descent(61/999): loss=0.48373261138615564\n",
      "Gradient Descent(62/999): loss=0.4837307071933746\n",
      "Gradient Descent(63/999): loss=0.48372897579474594\n",
      "Gradient Descent(64/999): loss=0.48372739842675155\n",
      "Gradient Descent(65/999): loss=0.48372595836338494\n",
      "Gradient Descent(66/999): loss=0.4837246406948998\n",
      "Gradient Descent(67/999): loss=0.4837234321305843\n",
      "Gradient Descent(68/999): loss=0.48372232082295197\n",
      "Gradient Descent(69/999): loss=0.48372129621102256\n",
      "Gradient Descent(70/999): loss=0.48372034888062243\n",
      "Gradient Descent(71/999): loss=0.48371947043985364\n",
      "Gradient Descent(72/999): loss=0.4837186534080858\n",
      "Gradient Descent(73/999): loss=0.4837178911170029\n",
      "Gradient Descent(74/999): loss=0.4837171776223946\n",
      "Gradient Descent(75/999): loss=0.48371650762552637\n",
      "Gradient Descent(76/999): loss=0.48371587640304814\n",
      "Gradient Descent(77/999): loss=0.48371527974451345\n",
      "Gradient Descent(78/999): loss=0.48371471389668425\n",
      "Gradient Descent(79/999): loss=0.4837141755138827\n",
      "Gradient Descent(80/999): loss=0.48371366161373464\n",
      "Gradient Descent(81/999): loss=0.4837131695377192\n",
      "Gradient Descent(82/999): loss=0.48371269691600155\n",
      "Gradient Descent(83/999): loss=0.48371224163608484\n",
      "Gradient Descent(84/999): loss=0.48371180181486606\n",
      "Gradient Descent(85/999): loss=0.4837113757737266\n",
      "Gradient Descent(86/999): loss=0.4837109620163272\n",
      "Gradient Descent(87/999): loss=0.4837105592088147\n",
      "Gradient Descent(88/999): loss=0.48371016616217777\n",
      "Gradient Descent(89/999): loss=0.48370978181651847\n",
      "Gradient Descent(90/999): loss=0.4837094052270322\n",
      "Gradient Descent(91/999): loss=0.4837090355515098\n",
      "Gradient Descent(92/999): loss=0.48370867203919676\n",
      "Gradient Descent(93/999): loss=0.48370831402086206\n",
      "Gradient Descent(94/999): loss=0.483707960899946\n",
      "Gradient Descent(95/999): loss=0.48370761214466873\n",
      "Gradient Descent(96/999): loss=0.4837072672809956\n",
      "Gradient Descent(97/999): loss=0.4837069258863673\n",
      "Gradient Descent(98/999): loss=0.4837065875841094\n",
      "Gradient Descent(99/999): loss=0.4837062520384502\n",
      "Gradient Descent(100/999): loss=0.48370591895007964\n",
      "Gradient Descent(101/999): loss=0.4837055880521888\n",
      "Gradient Descent(102/999): loss=0.4837052591069415\n",
      "Gradient Descent(103/999): loss=0.4837049319023281\n",
      "Gradient Descent(104/999): loss=0.4837046062493608\n",
      "Gradient Descent(105/999): loss=0.48370428197957355\n",
      "Gradient Descent(106/999): loss=0.4837039589427943\n",
      "Gradient Descent(107/999): loss=0.4837036370051576\n",
      "Gradient Descent(108/999): loss=0.48370331604733474\n",
      "Gradient Descent(109/999): loss=0.48370299596295424\n",
      "Gradient Descent(110/999): loss=0.48370267665719613\n",
      "Gradient Descent(111/999): loss=0.48370235804553613\n",
      "Gradient Descent(112/999): loss=0.4837020400526288\n",
      "Gradient Descent(113/999): loss=0.4837017226113103\n",
      "Gradient Descent(114/999): loss=0.48370140566170977\n",
      "Gradient Descent(115/999): loss=0.483701089150458\n",
      "Gradient Descent(116/999): loss=0.48370077302998127\n",
      "Gradient Descent(117/999): loss=0.4837004572578712\n",
      "Gradient Descent(118/999): loss=0.4837001417963251\n",
      "Gradient Descent(119/999): loss=0.4836998266116448\n",
      "Gradient Descent(120/999): loss=0.4836995116737916\n",
      "Gradient Descent(121/999): loss=0.48369919695598873\n",
      "Gradient Descent(122/999): loss=0.483698882434367\n",
      "Gradient Descent(123/999): loss=0.48369856808764916\n",
      "Gradient Descent(124/999): loss=0.4836982538968683\n",
      "Gradient Descent(125/999): loss=0.48369793984511733\n",
      "Gradient Descent(126/999): loss=0.4836976259173249\n",
      "Gradient Descent(127/999): loss=0.48369731210005645\n",
      "Gradient Descent(128/999): loss=0.4836969983813361\n",
      "Gradient Descent(129/999): loss=0.4836966847504884\n",
      "Gradient Descent(130/999): loss=0.48369637119799735\n",
      "Gradient Descent(131/999): loss=0.4836960577153799\n",
      "Gradient Descent(132/999): loss=0.48369574429507456\n",
      "Gradient Descent(133/999): loss=0.48369543093034073\n",
      "Gradient Descent(134/999): loss=0.48369511761516976\n",
      "Gradient Descent(135/999): loss=0.48369480434420564\n",
      "Gradient Descent(136/999): loss=0.4836944911126739\n",
      "Gradient Descent(137/999): loss=0.48369417791631814\n",
      "Gradient Descent(138/999): loss=0.4836938647513447\n",
      "Gradient Descent(139/999): loss=0.4836935516143718\n",
      "Gradient Descent(140/999): loss=0.48369323850238444\n",
      "Gradient Descent(141/999): loss=0.48369292541269543\n",
      "Gradient Descent(142/999): loss=0.48369261234290944\n",
      "Gradient Descent(143/999): loss=0.48369229929089086\n",
      "Gradient Descent(144/999): loss=0.48369198625473625\n",
      "Gradient Descent(145/999): loss=0.48369167323274886\n",
      "Gradient Descent(146/999): loss=0.4836913602234161\n",
      "Gradient Descent(147/999): loss=0.4836910472253896\n",
      "Gradient Descent(148/999): loss=0.48369073423746767\n",
      "Gradient Descent(149/999): loss=0.48369042125857875\n",
      "Gradient Descent(150/999): loss=0.4836901082877677\n",
      "Gradient Descent(151/999): loss=0.48368979532418355\n",
      "Gradient Descent(152/999): loss=0.4836894823670671\n",
      "Gradient Descent(153/999): loss=0.4836891694157419\n",
      "Gradient Descent(154/999): loss=0.4836888564696052\n",
      "Gradient Descent(155/999): loss=0.483688543528119\n",
      "Gradient Descent(156/999): loss=0.4836882305908045\n",
      "Gradient Descent(157/999): loss=0.4836879176572345\n",
      "Gradient Descent(158/999): loss=0.48368760472702815\n",
      "Gradient Descent(159/999): loss=0.48368729179984604\n",
      "Gradient Descent(160/999): loss=0.48368697887538575\n",
      "Gradient Descent(161/999): loss=0.48368666595337734\n",
      "Gradient Descent(162/999): loss=0.4836863530335807\n",
      "Gradient Descent(163/999): loss=0.48368604011578126\n",
      "Gradient Descent(164/999): loss=0.4836857271997882\n",
      "Gradient Descent(165/999): loss=0.48368541428543105\n",
      "Gradient Descent(166/999): loss=0.48368510137255816\n",
      "Gradient Descent(167/999): loss=0.4836847884610341\n",
      "Gradient Descent(168/999): loss=0.48368447555073824\n",
      "Gradient Descent(169/999): loss=0.4836841626415632\n",
      "Gradient Descent(170/999): loss=0.483683849733413\n",
      "Gradient Descent(171/999): loss=0.4836835368262022\n",
      "Gradient Descent(172/999): loss=0.48368322391985474\n",
      "Gradient Descent(173/999): loss=0.4836829110143026\n",
      "Gradient Descent(174/999): loss=0.4836825981094853\n",
      "Gradient Descent(175/999): loss=0.48368228520534906\n",
      "Gradient Descent(176/999): loss=0.4836819723018454\n",
      "Gradient Descent(177/999): loss=0.4836816593989319\n",
      "Gradient Descent(178/999): loss=0.48368134649657013\n",
      "Gradient Descent(179/999): loss=0.483681033594726\n",
      "Gradient Descent(180/999): loss=0.48368072069336915\n",
      "Gradient Descent(181/999): loss=0.4836804077924727\n",
      "Gradient Descent(182/999): loss=0.48368009489201225\n",
      "Gradient Descent(183/999): loss=0.48367978199196643\n",
      "Gradient Descent(184/999): loss=0.48367946909231607\n",
      "Gradient Descent(185/999): loss=0.483679156193044\n",
      "Gradient Descent(186/999): loss=0.4836788432941351\n",
      "Gradient Descent(187/999): loss=0.4836785303955758\n",
      "Gradient Descent(188/999): loss=0.48367821749735385\n",
      "Gradient Descent(189/999): loss=0.48367790459945864\n",
      "Gradient Descent(190/999): loss=0.4836775917018803\n",
      "Gradient Descent(191/999): loss=0.4836772788046104\n",
      "Gradient Descent(192/999): loss=0.4836769659076414\n",
      "Gradient Descent(193/999): loss=0.4836766530109665\n",
      "Gradient Descent(194/999): loss=0.4836763401145794\n",
      "Gradient Descent(195/999): loss=0.4836760272184747\n",
      "Gradient Descent(196/999): loss=0.48367571432264783\n",
      "Gradient Descent(197/999): loss=0.4836754014270942\n",
      "Gradient Descent(198/999): loss=0.48367508853181024\n",
      "Gradient Descent(199/999): loss=0.4836747756367924\n",
      "Gradient Descent(200/999): loss=0.48367446274203746\n",
      "Gradient Descent(201/999): loss=0.48367414984754303\n",
      "Gradient Descent(202/999): loss=0.48367383695330646\n",
      "Gradient Descent(203/999): loss=0.4836735240593257\n",
      "Gradient Descent(204/999): loss=0.48367321116559886\n",
      "Gradient Descent(205/999): loss=0.483672898272124\n",
      "Gradient Descent(206/999): loss=0.48367258537889984\n",
      "Gradient Descent(207/999): loss=0.4836722724859248\n",
      "Gradient Descent(208/999): loss=0.483671959593198\n",
      "Gradient Descent(209/999): loss=0.48367164670071805\n",
      "Gradient Descent(210/999): loss=0.4836713338084841\n",
      "Gradient Descent(211/999): loss=0.4836710209164951\n",
      "Gradient Descent(212/999): loss=0.48367070802475065\n",
      "Gradient Descent(213/999): loss=0.48367039513324983\n",
      "Gradient Descent(214/999): loss=0.483670082241992\n",
      "Gradient Descent(215/999): loss=0.4836697693509767\n",
      "Gradient Descent(216/999): loss=0.4836694564602035\n",
      "Gradient Descent(217/999): loss=0.48366914356967183\n",
      "Gradient Descent(218/999): loss=0.48366883067938143\n",
      "Gradient Descent(219/999): loss=0.4836685177893318\n",
      "Gradient Descent(220/999): loss=0.4836682048995228\n",
      "Gradient Descent(221/999): loss=0.4836678920099541\n",
      "Gradient Descent(222/999): loss=0.4836675791206254\n",
      "Gradient Descent(223/999): loss=0.48366726623153655\n",
      "Gradient Descent(224/999): loss=0.48366695334268733\n",
      "Gradient Descent(225/999): loss=0.48366664045407753\n",
      "Gradient Descent(226/999): loss=0.48366632756570704\n",
      "Gradient Descent(227/999): loss=0.4836660146775758\n",
      "Gradient Descent(228/999): loss=0.4836657017896836\n",
      "Gradient Descent(229/999): loss=0.48366538890203037\n",
      "Gradient Descent(230/999): loss=0.483665076014616\n",
      "Gradient Descent(231/999): loss=0.4836647631274403\n",
      "Gradient Descent(232/999): loss=0.48366445024050325\n",
      "Gradient Descent(233/999): loss=0.4836641373538049\n",
      "Gradient Descent(234/999): loss=0.4836638244673451\n",
      "Gradient Descent(235/999): loss=0.48366351158112375\n",
      "Gradient Descent(236/999): loss=0.4836631986951409\n",
      "Gradient Descent(237/999): loss=0.48366288580939637\n",
      "Gradient Descent(238/999): loss=0.4836625729238902\n",
      "Gradient Descent(239/999): loss=0.4836622600386225\n",
      "Gradient Descent(240/999): loss=0.48366194715359295\n",
      "Gradient Descent(241/999): loss=0.48366163426880177\n",
      "Gradient Descent(242/999): loss=0.48366132138424883\n",
      "Gradient Descent(243/999): loss=0.4836610084999341\n",
      "Gradient Descent(244/999): loss=0.4836606956158576\n",
      "Gradient Descent(245/999): loss=0.4836603827320192\n",
      "Gradient Descent(246/999): loss=0.4836600698484191\n",
      "Gradient Descent(247/999): loss=0.4836597569650571\n",
      "Gradient Descent(248/999): loss=0.4836594440819332\n",
      "Gradient Descent(249/999): loss=0.4836591311990475\n",
      "Gradient Descent(250/999): loss=0.4836588183163999\n",
      "Gradient Descent(251/999): loss=0.48365850543399036\n",
      "Gradient Descent(252/999): loss=0.483658192551819\n",
      "Gradient Descent(253/999): loss=0.4836578796698858\n",
      "Gradient Descent(254/999): loss=0.4836575667881906\n",
      "Gradient Descent(255/999): loss=0.4836572539067336\n",
      "Gradient Descent(256/999): loss=0.4836569410255146\n",
      "Gradient Descent(257/999): loss=0.48365662814453364\n",
      "Gradient Descent(258/999): loss=0.4836563152637908\n",
      "Gradient Descent(259/999): loss=0.48365600238328604\n",
      "Gradient Descent(260/999): loss=0.4836556895030193\n",
      "Gradient Descent(261/999): loss=0.4836553766229907\n",
      "Gradient Descent(262/999): loss=0.4836550637432001\n",
      "Gradient Descent(263/999): loss=0.4836547508636477\n",
      "Gradient Descent(264/999): loss=0.4836544379843333\n",
      "Gradient Descent(265/999): loss=0.48365412510525685\n",
      "Gradient Descent(266/999): loss=0.48365381222641846\n",
      "Gradient Descent(267/999): loss=0.48365349934781815\n",
      "Gradient Descent(268/999): loss=0.483653186469456\n",
      "Gradient Descent(269/999): loss=0.4836528735913318\n",
      "Gradient Descent(270/999): loss=0.48365256071344576\n",
      "Gradient Descent(271/999): loss=0.4836522478357977\n",
      "Gradient Descent(272/999): loss=0.4836519349583876\n",
      "Gradient Descent(273/999): loss=0.48365162208121565\n",
      "Gradient Descent(274/999): loss=0.48365130920428173\n",
      "Gradient Descent(275/999): loss=0.48365099632758574\n",
      "Gradient Descent(276/999): loss=0.48365068345112805\n",
      "Gradient Descent(277/999): loss=0.4836503705749083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(278/999): loss=0.4836500576989265\n",
      "Gradient Descent(279/999): loss=0.48364974482318285\n",
      "Gradient Descent(280/999): loss=0.4836494319476771\n",
      "Gradient Descent(281/999): loss=0.4836491190724095\n",
      "Gradient Descent(282/999): loss=0.48364880619738\n",
      "Gradient Descent(283/999): loss=0.48364849332258836\n",
      "Gradient Descent(284/999): loss=0.48364818044803487\n",
      "Gradient Descent(285/999): loss=0.48364786757371947\n",
      "Gradient Descent(286/999): loss=0.4836475546996421\n",
      "Gradient Descent(287/999): loss=0.48364724182580265\n",
      "Gradient Descent(288/999): loss=0.4836469289522014\n",
      "Gradient Descent(289/999): loss=0.4836466160788381\n",
      "Gradient Descent(290/999): loss=0.4836463032057129\n",
      "Gradient Descent(291/999): loss=0.48364599033282557\n",
      "Gradient Descent(292/999): loss=0.48364567746017645\n",
      "Gradient Descent(293/999): loss=0.48364536458776536\n",
      "Gradient Descent(294/999): loss=0.48364505171559224\n",
      "Gradient Descent(295/999): loss=0.48364473884365716\n",
      "Gradient Descent(296/999): loss=0.4836444259719602\n",
      "Gradient Descent(297/999): loss=0.4836441131005012\n",
      "Gradient Descent(298/999): loss=0.4836438002292803\n",
      "Gradient Descent(299/999): loss=0.4836434873582974\n",
      "Gradient Descent(300/999): loss=0.48364317448755256\n",
      "Gradient Descent(301/999): loss=0.48364286161704567\n",
      "Gradient Descent(302/999): loss=0.483642548746777\n",
      "Gradient Descent(303/999): loss=0.48364223587674615\n",
      "Gradient Descent(304/999): loss=0.48364192300695347\n",
      "Gradient Descent(305/999): loss=0.48364161013739865\n",
      "Gradient Descent(306/999): loss=0.483641297268082\n",
      "Gradient Descent(307/999): loss=0.48364098439900355\n",
      "Gradient Descent(308/999): loss=0.48364067153016294\n",
      "Gradient Descent(309/999): loss=0.48364035866156035\n",
      "Gradient Descent(310/999): loss=0.4836400457931959\n",
      "Gradient Descent(311/999): loss=0.48363973292506945\n",
      "Gradient Descent(312/999): loss=0.48363942005718097\n",
      "Gradient Descent(313/999): loss=0.4836391071895307\n",
      "Gradient Descent(314/999): loss=0.48363879432211826\n",
      "Gradient Descent(315/999): loss=0.4836384814549439\n",
      "Gradient Descent(316/999): loss=0.4836381685880076\n",
      "Gradient Descent(317/999): loss=0.48363785572130946\n",
      "Gradient Descent(318/999): loss=0.4836375428548492\n",
      "Gradient Descent(319/999): loss=0.483637229988627\n",
      "Gradient Descent(320/999): loss=0.4836369171226428\n",
      "Gradient Descent(321/999): loss=0.4836366042568967\n",
      "Gradient Descent(322/999): loss=0.48363629139138864\n",
      "Gradient Descent(323/999): loss=0.48363597852611856\n",
      "Gradient Descent(324/999): loss=0.48363566566108657\n",
      "Gradient Descent(325/999): loss=0.4836353527962926\n",
      "Gradient Descent(326/999): loss=0.48363503993173673\n",
      "Gradient Descent(327/999): loss=0.48363472706741867\n",
      "Gradient Descent(328/999): loss=0.48363441420333886\n",
      "Gradient Descent(329/999): loss=0.4836341013394969\n",
      "Gradient Descent(330/999): loss=0.4836337884758932\n",
      "Gradient Descent(331/999): loss=0.4836334756125273\n",
      "Gradient Descent(332/999): loss=0.4836331627493996\n",
      "Gradient Descent(333/999): loss=0.48363284988650984\n",
      "Gradient Descent(334/999): loss=0.4836325370238581\n",
      "Gradient Descent(335/999): loss=0.48363222416144447\n",
      "Gradient Descent(336/999): loss=0.4836319112992689\n",
      "Gradient Descent(337/999): loss=0.48363159843733117\n",
      "Gradient Descent(338/999): loss=0.4836312855756317\n",
      "Gradient Descent(339/999): loss=0.4836309727141701\n",
      "Gradient Descent(340/999): loss=0.4836306598529466\n",
      "Gradient Descent(341/999): loss=0.48363034699196106\n",
      "Gradient Descent(342/999): loss=0.4836300341312136\n",
      "Gradient Descent(343/999): loss=0.48362972127070425\n",
      "Gradient Descent(344/999): loss=0.48362940841043284\n",
      "Gradient Descent(345/999): loss=0.4836290955503994\n",
      "Gradient Descent(346/999): loss=0.4836287826906041\n",
      "Gradient Descent(347/999): loss=0.48362846983104685\n",
      "Gradient Descent(348/999): loss=0.48362815697172756\n",
      "Gradient Descent(349/999): loss=0.4836278441126462\n",
      "Gradient Descent(350/999): loss=0.483627531253803\n",
      "Gradient Descent(351/999): loss=0.4836272183951978\n",
      "Gradient Descent(352/999): loss=0.4836269055368307\n",
      "Gradient Descent(353/999): loss=0.4836265926787015\n",
      "Gradient Descent(354/999): loss=0.4836262798208104\n",
      "Gradient Descent(355/999): loss=0.48362596696315724\n",
      "Gradient Descent(356/999): loss=0.48362565410574226\n",
      "Gradient Descent(357/999): loss=0.4836253412485652\n",
      "Gradient Descent(358/999): loss=0.4836250283916262\n",
      "Gradient Descent(359/999): loss=0.48362471553492514\n",
      "Gradient Descent(360/999): loss=0.48362440267846224\n",
      "Gradient Descent(361/999): loss=0.48362408982223726\n",
      "Gradient Descent(362/999): loss=0.48362377696625036\n",
      "Gradient Descent(363/999): loss=0.4836234641105015\n",
      "Gradient Descent(364/999): loss=0.4836231512549906\n",
      "Gradient Descent(365/999): loss=0.48362283839971776\n",
      "Gradient Descent(366/999): loss=0.48362252554468305\n",
      "Gradient Descent(367/999): loss=0.48362221268988625\n",
      "Gradient Descent(368/999): loss=0.48362189983532744\n",
      "Gradient Descent(369/999): loss=0.48362158698100677\n",
      "Gradient Descent(370/999): loss=0.4836212741269241\n",
      "Gradient Descent(371/999): loss=0.4836209612730794\n",
      "Gradient Descent(372/999): loss=0.4836206484194728\n",
      "Gradient Descent(373/999): loss=0.48362033556610406\n",
      "Gradient Descent(374/999): loss=0.4836200227129735\n",
      "Gradient Descent(375/999): loss=0.4836197098600809\n",
      "Gradient Descent(376/999): loss=0.48361939700742623\n",
      "Gradient Descent(377/999): loss=0.4836190841550098\n",
      "Gradient Descent(378/999): loss=0.48361877130283126\n",
      "Gradient Descent(379/999): loss=0.48361845845089085\n",
      "Gradient Descent(380/999): loss=0.4836181455991883\n",
      "Gradient Descent(381/999): loss=0.48361783274772385\n",
      "Gradient Descent(382/999): loss=0.4836175198964975\n",
      "Gradient Descent(383/999): loss=0.483617207045509\n",
      "Gradient Descent(384/999): loss=0.4836168941947586\n",
      "Gradient Descent(385/999): loss=0.48361658134424634\n",
      "Gradient Descent(386/999): loss=0.483616268493972\n",
      "Gradient Descent(387/999): loss=0.48361595564393567\n",
      "Gradient Descent(388/999): loss=0.4836156427941373\n",
      "Gradient Descent(389/999): loss=0.483615329944577\n",
      "Gradient Descent(390/999): loss=0.48361501709525484\n",
      "Gradient Descent(391/999): loss=0.48361470424617053\n",
      "Gradient Descent(392/999): loss=0.4836143913973244\n",
      "Gradient Descent(393/999): loss=0.48361407854871613\n",
      "Gradient Descent(394/999): loss=0.483613765700346\n",
      "Gradient Descent(395/999): loss=0.48361345285221385\n",
      "Gradient Descent(396/999): loss=0.48361314000431976\n",
      "Gradient Descent(397/999): loss=0.48361282715666365\n",
      "Gradient Descent(398/999): loss=0.48361251430924557\n",
      "Gradient Descent(399/999): loss=0.4836122014620654\n",
      "Gradient Descent(400/999): loss=0.4836118886151233\n",
      "Gradient Descent(401/999): loss=0.48361157576841934\n",
      "Gradient Descent(402/999): loss=0.4836112629219534\n",
      "Gradient Descent(403/999): loss=0.4836109500757254\n",
      "Gradient Descent(404/999): loss=0.4836106372297354\n",
      "Gradient Descent(405/999): loss=0.48361032438398344\n",
      "Gradient Descent(406/999): loss=0.48361001153846944\n",
      "Gradient Descent(407/999): loss=0.48360969869319353\n",
      "Gradient Descent(408/999): loss=0.48360938584815566\n",
      "Gradient Descent(409/999): loss=0.48360907300335576\n",
      "Gradient Descent(410/999): loss=0.48360876015879384\n",
      "Gradient Descent(411/999): loss=0.48360844731447006\n",
      "Gradient Descent(412/999): loss=0.4836081344703842\n",
      "Gradient Descent(413/999): loss=0.4836078216265364\n",
      "Gradient Descent(414/999): loss=0.4836075087829266\n",
      "Gradient Descent(415/999): loss=0.48360719593955476\n",
      "Gradient Descent(416/999): loss=0.483606883096421\n",
      "Gradient Descent(417/999): loss=0.4836065702535252\n",
      "Gradient Descent(418/999): loss=0.4836062574108675\n",
      "Gradient Descent(419/999): loss=0.48360594456844774\n",
      "Gradient Descent(420/999): loss=0.4836056317262661\n",
      "Gradient Descent(421/999): loss=0.4836053188843224\n",
      "Gradient Descent(422/999): loss=0.4836050060426167\n",
      "Gradient Descent(423/999): loss=0.48360469320114907\n",
      "Gradient Descent(424/999): loss=0.4836043803599194\n",
      "Gradient Descent(425/999): loss=0.4836040675189277\n",
      "Gradient Descent(426/999): loss=0.4836037546781741\n",
      "Gradient Descent(427/999): loss=0.4836034418376586\n",
      "Gradient Descent(428/999): loss=0.4836031289973809\n",
      "Gradient Descent(429/999): loss=0.4836028161573414\n",
      "Gradient Descent(430/999): loss=0.4836025033175398\n",
      "Gradient Descent(431/999): loss=0.4836021904779762\n",
      "Gradient Descent(432/999): loss=0.4836018776386507\n",
      "Gradient Descent(433/999): loss=0.4836015647995631\n",
      "Gradient Descent(434/999): loss=0.4836012519607137\n",
      "Gradient Descent(435/999): loss=0.4836009391221021\n",
      "Gradient Descent(436/999): loss=0.48360062628372863\n",
      "Gradient Descent(437/999): loss=0.4836003134455932\n",
      "Gradient Descent(438/999): loss=0.4836000006076957\n",
      "Gradient Descent(439/999): loss=0.48359968777003626\n",
      "Gradient Descent(440/999): loss=0.4835993749326148\n",
      "Gradient Descent(441/999): loss=0.4835990620954314\n",
      "Gradient Descent(442/999): loss=0.483598749258486\n",
      "Gradient Descent(443/999): loss=0.4835984364217786\n",
      "Gradient Descent(444/999): loss=0.48359812358530924\n",
      "Gradient Descent(445/999): loss=0.48359781074907776\n",
      "Gradient Descent(446/999): loss=0.48359749791308443\n",
      "Gradient Descent(447/999): loss=0.4835971850773291\n",
      "Gradient Descent(448/999): loss=0.4835968722418117\n",
      "Gradient Descent(449/999): loss=0.4835965594065324\n",
      "Gradient Descent(450/999): loss=0.4835962465714911\n",
      "Gradient Descent(451/999): loss=0.48359593373668774\n",
      "Gradient Descent(452/999): loss=0.4835956209021225\n",
      "Gradient Descent(453/999): loss=0.4835953080677951\n",
      "Gradient Descent(454/999): loss=0.48359499523370575\n",
      "Gradient Descent(455/999): loss=0.48359468239985454\n",
      "Gradient Descent(456/999): loss=0.4835943695662413\n",
      "Gradient Descent(457/999): loss=0.4835940567328661\n",
      "Gradient Descent(458/999): loss=0.4835937438997287\n",
      "Gradient Descent(459/999): loss=0.48359343106682956\n",
      "Gradient Descent(460/999): loss=0.4835931182341683\n",
      "Gradient Descent(461/999): loss=0.48359280540174504\n",
      "Gradient Descent(462/999): loss=0.4835924925695599\n",
      "Gradient Descent(463/999): loss=0.4835921797376126\n",
      "Gradient Descent(464/999): loss=0.4835918669059034\n",
      "Gradient Descent(465/999): loss=0.4835915540744321\n",
      "Gradient Descent(466/999): loss=0.48359124124319897\n",
      "Gradient Descent(467/999): loss=0.4835909284122038\n",
      "Gradient Descent(468/999): loss=0.48359061558144667\n",
      "Gradient Descent(469/999): loss=0.48359030275092757\n",
      "Gradient Descent(470/999): loss=0.4835899899206463\n",
      "Gradient Descent(471/999): loss=0.4835896770906032\n",
      "Gradient Descent(472/999): loss=0.483589364260798\n",
      "Gradient Descent(473/999): loss=0.4835890514312308\n",
      "Gradient Descent(474/999): loss=0.4835887386019017\n",
      "Gradient Descent(475/999): loss=0.4835884257728106\n",
      "Gradient Descent(476/999): loss=0.48358811294395737\n",
      "Gradient Descent(477/999): loss=0.48358780011534225\n",
      "Gradient Descent(478/999): loss=0.4835874872869652\n",
      "Gradient Descent(479/999): loss=0.48358717445882604\n",
      "Gradient Descent(480/999): loss=0.4835868616309249\n",
      "Gradient Descent(481/999): loss=0.48358654880326185\n",
      "Gradient Descent(482/999): loss=0.4835862359758367\n",
      "Gradient Descent(483/999): loss=0.4835859231486496\n",
      "Gradient Descent(484/999): loss=0.48358561032170055\n",
      "Gradient Descent(485/999): loss=0.4835852974949894\n",
      "Gradient Descent(486/999): loss=0.48358498466851635\n",
      "Gradient Descent(487/999): loss=0.4835846718422813\n",
      "Gradient Descent(488/999): loss=0.48358435901628416\n",
      "Gradient Descent(489/999): loss=0.4835840461905251\n",
      "Gradient Descent(490/999): loss=0.483583733365004\n",
      "Gradient Descent(491/999): loss=0.4835834205397209\n",
      "Gradient Descent(492/999): loss=0.4835831077146759\n",
      "Gradient Descent(493/999): loss=0.48358279488986883\n",
      "Gradient Descent(494/999): loss=0.4835824820652997\n",
      "Gradient Descent(495/999): loss=0.4835821692409687\n",
      "Gradient Descent(496/999): loss=0.4835818564168756\n",
      "Gradient Descent(497/999): loss=0.48358154359302064\n",
      "Gradient Descent(498/999): loss=0.48358123076940357\n",
      "Gradient Descent(499/999): loss=0.4835809179460244\n",
      "Gradient Descent(500/999): loss=0.48358060512288337\n",
      "Gradient Descent(501/999): loss=0.4835802922999803\n",
      "Gradient Descent(502/999): loss=0.4835799794773153\n",
      "Gradient Descent(503/999): loss=0.4835796666548882\n",
      "Gradient Descent(504/999): loss=0.4835793538326991\n",
      "Gradient Descent(505/999): loss=0.4835790410107481\n",
      "Gradient Descent(506/999): loss=0.4835787281890351\n",
      "Gradient Descent(507/999): loss=0.48357841536755997\n",
      "Gradient Descent(508/999): loss=0.48357810254632294\n",
      "Gradient Descent(509/999): loss=0.4835777897253239\n",
      "Gradient Descent(510/999): loss=0.4835774769045627\n",
      "Gradient Descent(511/999): loss=0.4835771640840398\n",
      "Gradient Descent(512/999): loss=0.4835768512637547\n",
      "Gradient Descent(513/999): loss=0.4835765384437076\n",
      "Gradient Descent(514/999): loss=0.4835762256238986\n",
      "Gradient Descent(515/999): loss=0.48357591280432743\n",
      "Gradient Descent(516/999): loss=0.4835755999849944\n",
      "Gradient Descent(517/999): loss=0.4835752871658993\n",
      "Gradient Descent(518/999): loss=0.48357497434704216\n",
      "Gradient Descent(519/999): loss=0.48357466152842316\n",
      "Gradient Descent(520/999): loss=0.4835743487100422\n",
      "Gradient Descent(521/999): loss=0.48357403589189896\n",
      "Gradient Descent(522/999): loss=0.483573723073994\n",
      "Gradient Descent(523/999): loss=0.4835734102563269\n",
      "Gradient Descent(524/999): loss=0.4835730974388978\n",
      "Gradient Descent(525/999): loss=0.4835727846217067\n",
      "Gradient Descent(526/999): loss=0.4835724718047535\n",
      "Gradient Descent(527/999): loss=0.4835721589880384\n",
      "Gradient Descent(528/999): loss=0.48357184617156135\n",
      "Gradient Descent(529/999): loss=0.4835715333553222\n",
      "Gradient Descent(530/999): loss=0.48357122053932106\n",
      "Gradient Descent(531/999): loss=0.48357090772355804\n",
      "Gradient Descent(532/999): loss=0.48357059490803295\n",
      "Gradient Descent(533/999): loss=0.4835702820927458\n",
      "Gradient Descent(534/999): loss=0.4835699692776966\n",
      "Gradient Descent(535/999): loss=0.4835696564628855\n",
      "Gradient Descent(536/999): loss=0.48356934364831233\n",
      "Gradient Descent(537/999): loss=0.4835690308339772\n",
      "Gradient Descent(538/999): loss=0.48356871801988\n",
      "Gradient Descent(539/999): loss=0.4835684052060209\n",
      "Gradient Descent(540/999): loss=0.48356809239239973\n",
      "Gradient Descent(541/999): loss=0.4835677795790165\n",
      "Gradient Descent(542/999): loss=0.48356746676587137\n",
      "Gradient Descent(543/999): loss=0.48356715395296423\n",
      "Gradient Descent(544/999): loss=0.48356684114029497\n",
      "Gradient Descent(545/999): loss=0.4835665283278638\n",
      "Gradient Descent(546/999): loss=0.4835662155156706\n",
      "Gradient Descent(547/999): loss=0.48356590270371536\n",
      "Gradient Descent(548/999): loss=0.4835655898919981\n",
      "Gradient Descent(549/999): loss=0.4835652770805189\n",
      "Gradient Descent(550/999): loss=0.4835649642692777\n",
      "Gradient Descent(551/999): loss=0.48356465145827443\n",
      "Gradient Descent(552/999): loss=0.48356433864750914\n",
      "Gradient Descent(553/999): loss=0.48356402583698194\n",
      "Gradient Descent(554/999): loss=0.4835637130266926\n",
      "Gradient Descent(555/999): loss=0.4835634002166413\n",
      "Gradient Descent(556/999): loss=0.4835630874068281\n",
      "Gradient Descent(557/999): loss=0.4835627745972527\n",
      "Gradient Descent(558/999): loss=0.48356246178791545\n",
      "Gradient Descent(559/999): loss=0.4835621489788161\n",
      "Gradient Descent(560/999): loss=0.4835618361699548\n",
      "Gradient Descent(561/999): loss=0.48356152336133146\n",
      "Gradient Descent(562/999): loss=0.48356121055294604\n",
      "Gradient Descent(563/999): loss=0.48356089774479877\n",
      "Gradient Descent(564/999): loss=0.48356058493688936\n",
      "Gradient Descent(565/999): loss=0.483560272129218\n",
      "Gradient Descent(566/999): loss=0.4835599593217846\n",
      "Gradient Descent(567/999): loss=0.48355964651458916\n",
      "Gradient Descent(568/999): loss=0.4835593337076317\n",
      "Gradient Descent(569/999): loss=0.4835590209009123\n",
      "Gradient Descent(570/999): loss=0.48355870809443086\n",
      "Gradient Descent(571/999): loss=0.48355839528818745\n",
      "Gradient Descent(572/999): loss=0.483558082482182\n",
      "Gradient Descent(573/999): loss=0.4835577696764145\n",
      "Gradient Descent(574/999): loss=0.483557456870885\n",
      "Gradient Descent(575/999): loss=0.48355714406559347\n",
      "Gradient Descent(576/999): loss=0.48355683126054\n",
      "Gradient Descent(577/999): loss=0.48355651845572445\n",
      "Gradient Descent(578/999): loss=0.48355620565114693\n",
      "Gradient Descent(579/999): loss=0.4835558928468074\n",
      "Gradient Descent(580/999): loss=0.48355558004270577\n",
      "Gradient Descent(581/999): loss=0.48355526723884223\n",
      "Gradient Descent(582/999): loss=0.48355495443521657\n",
      "Gradient Descent(583/999): loss=0.483554641631829\n",
      "Gradient Descent(584/999): loss=0.48355432882867944\n",
      "Gradient Descent(585/999): loss=0.48355401602576775\n",
      "Gradient Descent(586/999): loss=0.483553703223094\n",
      "Gradient Descent(587/999): loss=0.48355339042065837\n",
      "Gradient Descent(588/999): loss=0.4835530776184607\n",
      "Gradient Descent(589/999): loss=0.483552764816501\n",
      "Gradient Descent(590/999): loss=0.4835524520147793\n",
      "Gradient Descent(591/999): loss=0.4835521392132956\n",
      "Gradient Descent(592/999): loss=0.48355182641204986\n",
      "Gradient Descent(593/999): loss=0.4835515136110421\n",
      "Gradient Descent(594/999): loss=0.48355120081027236\n",
      "Gradient Descent(595/999): loss=0.48355088800974055\n",
      "Gradient Descent(596/999): loss=0.48355057520944666\n",
      "Gradient Descent(597/999): loss=0.48355026240939086\n",
      "Gradient Descent(598/999): loss=0.48354994960957304\n",
      "Gradient Descent(599/999): loss=0.4835496368099932\n",
      "Gradient Descent(600/999): loss=0.48354932401065126\n",
      "Gradient Descent(601/999): loss=0.4835490112115474\n",
      "Gradient Descent(602/999): loss=0.48354869841268144\n",
      "Gradient Descent(603/999): loss=0.4835483856140536\n",
      "Gradient Descent(604/999): loss=0.48354807281566364\n",
      "Gradient Descent(605/999): loss=0.4835477600175116\n",
      "Gradient Descent(606/999): loss=0.4835474472195977\n",
      "Gradient Descent(607/999): loss=0.4835471344219217\n",
      "Gradient Descent(608/999): loss=0.4835468216244836\n",
      "Gradient Descent(609/999): loss=0.48354650882728367\n",
      "Gradient Descent(610/999): loss=0.4835461960303215\n",
      "Gradient Descent(611/999): loss=0.4835458832335975\n",
      "Gradient Descent(612/999): loss=0.4835455704371114\n",
      "Gradient Descent(613/999): loss=0.4835452576408633\n",
      "Gradient Descent(614/999): loss=0.4835449448448531\n",
      "Gradient Descent(615/999): loss=0.48354463204908105\n",
      "Gradient Descent(616/999): loss=0.48354431925354674\n",
      "Gradient Descent(617/999): loss=0.48354400645825063\n",
      "Gradient Descent(618/999): loss=0.48354369366319244\n",
      "Gradient Descent(619/999): loss=0.4835433808683722\n",
      "Gradient Descent(620/999): loss=0.4835430680737899\n",
      "Gradient Descent(621/999): loss=0.48354275527944574\n",
      "Gradient Descent(622/999): loss=0.48354244248533934\n",
      "Gradient Descent(623/999): loss=0.48354212969147103\n",
      "Gradient Descent(624/999): loss=0.4835418168978407\n",
      "Gradient Descent(625/999): loss=0.4835415041044483\n",
      "Gradient Descent(626/999): loss=0.4835411913112939\n",
      "Gradient Descent(627/999): loss=0.48354087851837746\n",
      "Gradient Descent(628/999): loss=0.4835405657256991\n",
      "Gradient Descent(629/999): loss=0.4835402529332587\n",
      "Gradient Descent(630/999): loss=0.4835399401410561\n",
      "Gradient Descent(631/999): loss=0.4835396273490917\n",
      "Gradient Descent(632/999): loss=0.4835393145573652\n",
      "Gradient Descent(633/999): loss=0.4835390017658766\n",
      "Gradient Descent(634/999): loss=0.4835386889746261\n",
      "Gradient Descent(635/999): loss=0.4835383761836135\n",
      "Gradient Descent(636/999): loss=0.48353806339283883\n",
      "Gradient Descent(637/999): loss=0.4835377506023022\n",
      "Gradient Descent(638/999): loss=0.48353743781200353\n",
      "Gradient Descent(639/999): loss=0.48353712502194285\n",
      "Gradient Descent(640/999): loss=0.48353681223212014\n",
      "Gradient Descent(641/999): loss=0.4835364994425354\n",
      "Gradient Descent(642/999): loss=0.48353618665318865\n",
      "Gradient Descent(643/999): loss=0.48353587386407987\n",
      "Gradient Descent(644/999): loss=0.48353556107520906\n",
      "Gradient Descent(645/999): loss=0.4835352482865763\n",
      "Gradient Descent(646/999): loss=0.4835349354981814\n",
      "Gradient Descent(647/999): loss=0.48353462271002445\n",
      "Gradient Descent(648/999): loss=0.4835343099221056\n",
      "Gradient Descent(649/999): loss=0.4835339971344247\n",
      "Gradient Descent(650/999): loss=0.4835336843469817\n",
      "Gradient Descent(651/999): loss=0.4835333715597767\n",
      "Gradient Descent(652/999): loss=0.4835330587728096\n",
      "Gradient Descent(653/999): loss=0.4835327459860806\n",
      "Gradient Descent(654/999): loss=0.48353243319958955\n",
      "Gradient Descent(655/999): loss=0.48353212041333643\n",
      "Gradient Descent(656/999): loss=0.48353180762732134\n",
      "Gradient Descent(657/999): loss=0.4835314948415442\n",
      "Gradient Descent(658/999): loss=0.4835311820560049\n",
      "Gradient Descent(659/999): loss=0.4835308692707038\n",
      "Gradient Descent(660/999): loss=0.4835305564856405\n",
      "Gradient Descent(661/999): loss=0.4835302437008153\n",
      "Gradient Descent(662/999): loss=0.48352993091622803\n",
      "Gradient Descent(663/999): loss=0.48352961813187867\n",
      "Gradient Descent(664/999): loss=0.48352930534776734\n",
      "Gradient Descent(665/999): loss=0.4835289925638939\n",
      "Gradient Descent(666/999): loss=0.48352867978025854\n",
      "Gradient Descent(667/999): loss=0.483528366996861\n",
      "Gradient Descent(668/999): loss=0.48352805421370165\n",
      "Gradient Descent(669/999): loss=0.48352774143078014\n",
      "Gradient Descent(670/999): loss=0.4835274286480967\n",
      "Gradient Descent(671/999): loss=0.4835271158656511\n",
      "Gradient Descent(672/999): loss=0.48352680308344353\n",
      "Gradient Descent(673/999): loss=0.48352649030147393\n",
      "Gradient Descent(674/999): loss=0.4835261775197422\n",
      "Gradient Descent(675/999): loss=0.4835258647382486\n",
      "Gradient Descent(676/999): loss=0.48352555195699287\n",
      "Gradient Descent(677/999): loss=0.4835252391759752\n",
      "Gradient Descent(678/999): loss=0.48352492639519534\n",
      "Gradient Descent(679/999): loss=0.4835246136146535\n",
      "Gradient Descent(680/999): loss=0.48352430083434966\n",
      "Gradient Descent(681/999): loss=0.4835239880542838\n",
      "Gradient Descent(682/999): loss=0.48352367527445594\n",
      "Gradient Descent(683/999): loss=0.48352336249486605\n",
      "Gradient Descent(684/999): loss=0.483523049715514\n",
      "Gradient Descent(685/999): loss=0.4835227369364\n",
      "Gradient Descent(686/999): loss=0.48352242415752406\n",
      "Gradient Descent(687/999): loss=0.48352211137888607\n",
      "Gradient Descent(688/999): loss=0.4835217986004859\n",
      "Gradient Descent(689/999): loss=0.48352148582232374\n",
      "Gradient Descent(690/999): loss=0.4835211730443995\n",
      "Gradient Descent(691/999): loss=0.4835208602667135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(692/999): loss=0.4835205474892652\n",
      "Gradient Descent(693/999): loss=0.4835202347120549\n",
      "Gradient Descent(694/999): loss=0.4835199219350827\n",
      "Gradient Descent(695/999): loss=0.48351960915834835\n",
      "Gradient Descent(696/999): loss=0.48351929638185204\n",
      "Gradient Descent(697/999): loss=0.48351898360559353\n",
      "Gradient Descent(698/999): loss=0.48351867082957317\n",
      "Gradient Descent(699/999): loss=0.4835183580537907\n",
      "Gradient Descent(700/999): loss=0.48351804527824616\n",
      "Gradient Descent(701/999): loss=0.48351773250293956\n",
      "Gradient Descent(702/999): loss=0.48351741972787105\n",
      "Gradient Descent(703/999): loss=0.48351710695304034\n",
      "Gradient Descent(704/999): loss=0.4835167941784478\n",
      "Gradient Descent(705/999): loss=0.4835164814040931\n",
      "Gradient Descent(706/999): loss=0.4835161686299764\n",
      "Gradient Descent(707/999): loss=0.4835158558560976\n",
      "Gradient Descent(708/999): loss=0.48351554308245687\n",
      "Gradient Descent(709/999): loss=0.483515230309054\n",
      "Gradient Descent(710/999): loss=0.48351491753588904\n",
      "Gradient Descent(711/999): loss=0.48351460476296226\n",
      "Gradient Descent(712/999): loss=0.48351429199027324\n",
      "Gradient Descent(713/999): loss=0.48351397921782235\n",
      "Gradient Descent(714/999): loss=0.4835136664456093\n",
      "Gradient Descent(715/999): loss=0.48351335367363424\n",
      "Gradient Descent(716/999): loss=0.4835130409018972\n",
      "Gradient Descent(717/999): loss=0.48351272813039803\n",
      "Gradient Descent(718/999): loss=0.48351241535913675\n",
      "Gradient Descent(719/999): loss=0.4835121025881137\n",
      "Gradient Descent(720/999): loss=0.48351178981732845\n",
      "Gradient Descent(721/999): loss=0.48351147704678116\n",
      "Gradient Descent(722/999): loss=0.4835111642764717\n",
      "Gradient Descent(723/999): loss=0.48351085150640044\n",
      "Gradient Descent(724/999): loss=0.483510538736567\n",
      "Gradient Descent(725/999): loss=0.4835102259669716\n",
      "Gradient Descent(726/999): loss=0.48350991319761416\n",
      "Gradient Descent(727/999): loss=0.48350960042849467\n",
      "Gradient Descent(728/999): loss=0.483509287659613\n",
      "Gradient Descent(729/999): loss=0.4835089748909695\n",
      "Gradient Descent(730/999): loss=0.48350866212256377\n",
      "Gradient Descent(731/999): loss=0.48350834935439607\n",
      "Gradient Descent(732/999): loss=0.4835080365864663\n",
      "Gradient Descent(733/999): loss=0.4835077238187746\n",
      "Gradient Descent(734/999): loss=0.48350741105132083\n",
      "Gradient Descent(735/999): loss=0.48350709828410493\n",
      "Gradient Descent(736/999): loss=0.483506785517127\n",
      "Gradient Descent(737/999): loss=0.48350647275038716\n",
      "Gradient Descent(738/999): loss=0.48350615998388513\n",
      "Gradient Descent(739/999): loss=0.48350584721762113\n",
      "Gradient Descent(740/999): loss=0.48350553445159505\n",
      "Gradient Descent(741/999): loss=0.483505221685807\n",
      "Gradient Descent(742/999): loss=0.4835049089202568\n",
      "Gradient Descent(743/999): loss=0.48350459615494457\n",
      "Gradient Descent(744/999): loss=0.4835042833898704\n",
      "Gradient Descent(745/999): loss=0.48350397062503414\n",
      "Gradient Descent(746/999): loss=0.48350365786043586\n",
      "Gradient Descent(747/999): loss=0.48350334509607545\n",
      "Gradient Descent(748/999): loss=0.483503032331953\n",
      "Gradient Descent(749/999): loss=0.48350271956806856\n",
      "Gradient Descent(750/999): loss=0.483502406804422\n",
      "Gradient Descent(751/999): loss=0.4835020940410135\n",
      "Gradient Descent(752/999): loss=0.48350178127784293\n",
      "Gradient Descent(753/999): loss=0.48350146851491027\n",
      "Gradient Descent(754/999): loss=0.4835011557522157\n",
      "Gradient Descent(755/999): loss=0.48350084298975887\n",
      "Gradient Descent(756/999): loss=0.4835005302275402\n",
      "Gradient Descent(757/999): loss=0.4835002174655594\n",
      "Gradient Descent(758/999): loss=0.48349990470381654\n",
      "Gradient Descent(759/999): loss=0.4834995919423116\n",
      "Gradient Descent(760/999): loss=0.4834992791810447\n",
      "Gradient Descent(761/999): loss=0.48349896642001566\n",
      "Gradient Descent(762/999): loss=0.4834986536592246\n",
      "Gradient Descent(763/999): loss=0.48349834089867155\n",
      "Gradient Descent(764/999): loss=0.4834980281383564\n",
      "Gradient Descent(765/999): loss=0.4834977153782792\n",
      "Gradient Descent(766/999): loss=0.48349740261844\n",
      "Gradient Descent(767/999): loss=0.4834970898588387\n",
      "Gradient Descent(768/999): loss=0.48349677709947536\n",
      "Gradient Descent(769/999): loss=0.48349646434035004\n",
      "Gradient Descent(770/999): loss=0.48349615158146264\n",
      "Gradient Descent(771/999): loss=0.4834958388228131\n",
      "Gradient Descent(772/999): loss=0.48349552606440155\n",
      "Gradient Descent(773/999): loss=0.483495213306228\n",
      "Gradient Descent(774/999): loss=0.4834949005482924\n",
      "Gradient Descent(775/999): loss=0.4834945877905947\n",
      "Gradient Descent(776/999): loss=0.4834942750331351\n",
      "Gradient Descent(777/999): loss=0.48349396227591335\n",
      "Gradient Descent(778/999): loss=0.4834936495189295\n",
      "Gradient Descent(779/999): loss=0.48349333676218365\n",
      "Gradient Descent(780/999): loss=0.4834930240056758\n",
      "Gradient Descent(781/999): loss=0.4834927112494058\n",
      "Gradient Descent(782/999): loss=0.4834923984933737\n",
      "Gradient Descent(783/999): loss=0.4834920857375797\n",
      "Gradient Descent(784/999): loss=0.48349177298202356\n",
      "Gradient Descent(785/999): loss=0.48349146022670536\n",
      "Gradient Descent(786/999): loss=0.4834911474716252\n",
      "Gradient Descent(787/999): loss=0.48349083471678295\n",
      "Gradient Descent(788/999): loss=0.4834905219621786\n",
      "Gradient Descent(789/999): loss=0.4834902092078123\n",
      "Gradient Descent(790/999): loss=0.48348989645368384\n",
      "Gradient Descent(791/999): loss=0.4834895836997934\n",
      "Gradient Descent(792/999): loss=0.4834892709461408\n",
      "Gradient Descent(793/999): loss=0.4834889581927263\n",
      "Gradient Descent(794/999): loss=0.4834886454395496\n",
      "Gradient Descent(795/999): loss=0.4834883326866109\n",
      "Gradient Descent(796/999): loss=0.4834880199339103\n",
      "Gradient Descent(797/999): loss=0.4834877071814475\n",
      "Gradient Descent(798/999): loss=0.4834873944292226\n",
      "Gradient Descent(799/999): loss=0.4834870816772358\n",
      "Gradient Descent(800/999): loss=0.4834867689254868\n",
      "Gradient Descent(801/999): loss=0.4834864561739759\n",
      "Gradient Descent(802/999): loss=0.48348614342270285\n",
      "Gradient Descent(803/999): loss=0.48348583067166767\n",
      "Gradient Descent(804/999): loss=0.4834855179208706\n",
      "Gradient Descent(805/999): loss=0.4834852051703114\n",
      "Gradient Descent(806/999): loss=0.48348489241999004\n",
      "Gradient Descent(807/999): loss=0.4834845796699067\n",
      "Gradient Descent(808/999): loss=0.4834842669200613\n",
      "Gradient Descent(809/999): loss=0.4834839541704539\n",
      "Gradient Descent(810/999): loss=0.48348364142108446\n",
      "Gradient Descent(811/999): loss=0.4834833286719529\n",
      "Gradient Descent(812/999): loss=0.4834830159230593\n",
      "Gradient Descent(813/999): loss=0.48348270317440367\n",
      "Gradient Descent(814/999): loss=0.48348239042598595\n",
      "Gradient Descent(815/999): loss=0.4834820776778062\n",
      "Gradient Descent(816/999): loss=0.4834817649298644\n",
      "Gradient Descent(817/999): loss=0.4834814521821605\n",
      "Gradient Descent(818/999): loss=0.4834811394346946\n",
      "Gradient Descent(819/999): loss=0.48348082668746656\n",
      "Gradient Descent(820/999): loss=0.48348051394047653\n",
      "Gradient Descent(821/999): loss=0.4834802011937244\n",
      "Gradient Descent(822/999): loss=0.4834798884472103\n",
      "Gradient Descent(823/999): loss=0.48347957570093414\n",
      "Gradient Descent(824/999): loss=0.4834792629548958\n",
      "Gradient Descent(825/999): loss=0.4834789502090955\n",
      "Gradient Descent(826/999): loss=0.48347863746353303\n",
      "Gradient Descent(827/999): loss=0.4834783247182087\n",
      "Gradient Descent(828/999): loss=0.4834780119731221\n",
      "Gradient Descent(829/999): loss=0.48347769922827355\n",
      "Gradient Descent(830/999): loss=0.48347738648366295\n",
      "Gradient Descent(831/999): loss=0.4834770737392902\n",
      "Gradient Descent(832/999): loss=0.48347676099515546\n",
      "Gradient Descent(833/999): loss=0.48347644825125874\n",
      "Gradient Descent(834/999): loss=0.48347613550759977\n",
      "Gradient Descent(835/999): loss=0.4834758227641789\n",
      "Gradient Descent(836/999): loss=0.483475510020996\n",
      "Gradient Descent(837/999): loss=0.48347519727805083\n",
      "Gradient Descent(838/999): loss=0.48347488453534376\n",
      "Gradient Descent(839/999): loss=0.48347457179287456\n",
      "Gradient Descent(840/999): loss=0.4834742590506434\n",
      "Gradient Descent(841/999): loss=0.4834739463086501\n",
      "Gradient Descent(842/999): loss=0.48347363356689477\n",
      "Gradient Descent(843/999): loss=0.4834733208253773\n",
      "Gradient Descent(844/999): loss=0.48347300808409793\n",
      "Gradient Descent(845/999): loss=0.4834726953430564\n",
      "Gradient Descent(846/999): loss=0.48347238260225284\n",
      "Gradient Descent(847/999): loss=0.4834720698616871\n",
      "Gradient Descent(848/999): loss=0.4834717571213594\n",
      "Gradient Descent(849/999): loss=0.4834714443812696\n",
      "Gradient Descent(850/999): loss=0.48347113164141775\n",
      "Gradient Descent(851/999): loss=0.4834708189018039\n",
      "Gradient Descent(852/999): loss=0.483470506162428\n",
      "Gradient Descent(853/999): loss=0.48347019342328984\n",
      "Gradient Descent(854/999): loss=0.48346988068438984\n",
      "Gradient Descent(855/999): loss=0.4834695679457277\n",
      "Gradient Descent(856/999): loss=0.4834692552073035\n",
      "Gradient Descent(857/999): loss=0.48346894246911715\n",
      "Gradient Descent(858/999): loss=0.4834686297311688\n",
      "Gradient Descent(859/999): loss=0.4834683169934585\n",
      "Gradient Descent(860/999): loss=0.48346800425598596\n",
      "Gradient Descent(861/999): loss=0.4834676915187515\n",
      "Gradient Descent(862/999): loss=0.48346737878175483\n",
      "Gradient Descent(863/999): loss=0.4834670660449961\n",
      "Gradient Descent(864/999): loss=0.4834667533084754\n",
      "Gradient Descent(865/999): loss=0.4834664405721926\n",
      "Gradient Descent(866/999): loss=0.48346612783614773\n",
      "Gradient Descent(867/999): loss=0.48346581510034087\n",
      "Gradient Descent(868/999): loss=0.48346550236477176\n",
      "Gradient Descent(869/999): loss=0.48346518962944085\n",
      "Gradient Descent(870/999): loss=0.4834648768943477\n",
      "Gradient Descent(871/999): loss=0.4834645641594925\n",
      "Gradient Descent(872/999): loss=0.48346425142487515\n",
      "Gradient Descent(873/999): loss=0.48346393869049586\n",
      "Gradient Descent(874/999): loss=0.48346362595635456\n",
      "Gradient Descent(875/999): loss=0.48346331322245095\n",
      "Gradient Descent(876/999): loss=0.4834630004887855\n",
      "Gradient Descent(877/999): loss=0.4834626877553579\n",
      "Gradient Descent(878/999): loss=0.48346237502216827\n",
      "Gradient Descent(879/999): loss=0.48346206228921657\n",
      "Gradient Descent(880/999): loss=0.4834617495565028\n",
      "Gradient Descent(881/999): loss=0.48346143682402687\n",
      "Gradient Descent(882/999): loss=0.48346112409178893\n",
      "Gradient Descent(883/999): loss=0.4834608113597889\n",
      "Gradient Descent(884/999): loss=0.4834604986280268\n",
      "Gradient Descent(885/999): loss=0.48346018589650286\n",
      "Gradient Descent(886/999): loss=0.4834598731652166\n",
      "Gradient Descent(887/999): loss=0.48345956043416827\n",
      "Gradient Descent(888/999): loss=0.48345924770335796\n",
      "Gradient Descent(889/999): loss=0.48345893497278547\n",
      "Gradient Descent(890/999): loss=0.483458622242451\n",
      "Gradient Descent(891/999): loss=0.48345830951235447\n",
      "Gradient Descent(892/999): loss=0.48345799678249585\n",
      "Gradient Descent(893/999): loss=0.48345768405287515\n",
      "Gradient Descent(894/999): loss=0.4834573713234924\n",
      "Gradient Descent(895/999): loss=0.4834570585943477\n",
      "Gradient Descent(896/999): loss=0.48345674586544063\n",
      "Gradient Descent(897/999): loss=0.48345643313677167\n",
      "Gradient Descent(898/999): loss=0.4834561204083407\n",
      "Gradient Descent(899/999): loss=0.4834558076801475\n",
      "Gradient Descent(900/999): loss=0.48345549495219237\n",
      "Gradient Descent(901/999): loss=0.4834551822244751\n",
      "Gradient Descent(902/999): loss=0.4834548694969958\n",
      "Gradient Descent(903/999): loss=0.48345455676975435\n",
      "Gradient Descent(904/999): loss=0.48345424404275084\n",
      "Gradient Descent(905/999): loss=0.4834539313159853\n",
      "Gradient Descent(906/999): loss=0.4834536185894578\n",
      "Gradient Descent(907/999): loss=0.48345330586316804\n",
      "Gradient Descent(908/999): loss=0.4834529931371162\n",
      "Gradient Descent(909/999): loss=0.4834526804113024\n",
      "Gradient Descent(910/999): loss=0.48345236768572647\n",
      "Gradient Descent(911/999): loss=0.4834520549603885\n",
      "Gradient Descent(912/999): loss=0.4834517422352885\n",
      "Gradient Descent(913/999): loss=0.48345142951042636\n",
      "Gradient Descent(914/999): loss=0.48345111678580205\n",
      "Gradient Descent(915/999): loss=0.4834508040614159\n",
      "Gradient Descent(916/999): loss=0.4834504913372674\n",
      "Gradient Descent(917/999): loss=0.483450178613357\n",
      "Gradient Descent(918/999): loss=0.4834498658896846\n",
      "Gradient Descent(919/999): loss=0.48344955316625\n",
      "Gradient Descent(920/999): loss=0.48344924044305326\n",
      "Gradient Descent(921/999): loss=0.4834489277200946\n",
      "Gradient Descent(922/999): loss=0.4834486149973738\n",
      "Gradient Descent(923/999): loss=0.48344830227489094\n",
      "Gradient Descent(924/999): loss=0.48344798955264584\n",
      "Gradient Descent(925/999): loss=0.48344767683063894\n",
      "Gradient Descent(926/999): loss=0.48344736410886974\n",
      "Gradient Descent(927/999): loss=0.4834470513873385\n",
      "Gradient Descent(928/999): loss=0.4834467386660454\n",
      "Gradient Descent(929/999): loss=0.48344642594499\n",
      "Gradient Descent(930/999): loss=0.4834461132241726\n",
      "Gradient Descent(931/999): loss=0.4834458005035931\n",
      "Gradient Descent(932/999): loss=0.4834454877832515\n",
      "Gradient Descent(933/999): loss=0.4834451750631479\n",
      "Gradient Descent(934/999): loss=0.48344486234328216\n",
      "Gradient Descent(935/999): loss=0.48344454962365435\n",
      "Gradient Descent(936/999): loss=0.48344423690426447\n",
      "Gradient Descent(937/999): loss=0.4834439241851125\n",
      "Gradient Descent(938/999): loss=0.48344361146619846\n",
      "Gradient Descent(939/999): loss=0.4834432987475224\n",
      "Gradient Descent(940/999): loss=0.4834429860290842\n",
      "Gradient Descent(941/999): loss=0.483442673310884\n",
      "Gradient Descent(942/999): loss=0.4834423605929216\n",
      "Gradient Descent(943/999): loss=0.483442047875197\n",
      "Gradient Descent(944/999): loss=0.48344173515771055\n",
      "Gradient Descent(945/999): loss=0.48344142244046195\n",
      "Gradient Descent(946/999): loss=0.48344110972345133\n",
      "Gradient Descent(947/999): loss=0.48344079700667864\n",
      "Gradient Descent(948/999): loss=0.48344048429014375\n",
      "Gradient Descent(949/999): loss=0.4834401715738468\n",
      "Gradient Descent(950/999): loss=0.48343985885778784\n",
      "Gradient Descent(951/999): loss=0.4834395461419668\n",
      "Gradient Descent(952/999): loss=0.4834392334263836\n",
      "Gradient Descent(953/999): loss=0.4834389207110384\n",
      "Gradient Descent(954/999): loss=0.483438607995931\n",
      "Gradient Descent(955/999): loss=0.48343829528106164\n",
      "Gradient Descent(956/999): loss=0.4834379825664302\n",
      "Gradient Descent(957/999): loss=0.4834376698520367\n",
      "Gradient Descent(958/999): loss=0.483437357137881\n",
      "Gradient Descent(959/999): loss=0.4834370444239633\n",
      "Gradient Descent(960/999): loss=0.4834367317102835\n",
      "Gradient Descent(961/999): loss=0.48343641899684164\n",
      "Gradient Descent(962/999): loss=0.48343610628363765\n",
      "Gradient Descent(963/999): loss=0.48343579357067157\n",
      "Gradient Descent(964/999): loss=0.48343548085794347\n",
      "Gradient Descent(965/999): loss=0.4834351681454532\n",
      "Gradient Descent(966/999): loss=0.4834348554332009\n",
      "Gradient Descent(967/999): loss=0.4834345427211865\n",
      "Gradient Descent(968/999): loss=0.48343423000941005\n",
      "Gradient Descent(969/999): loss=0.48343391729787144\n",
      "Gradient Descent(970/999): loss=0.4834336045865708\n",
      "Gradient Descent(971/999): loss=0.48343329187550815\n",
      "Gradient Descent(972/999): loss=0.48343297916468325\n",
      "Gradient Descent(973/999): loss=0.4834326664540964\n",
      "Gradient Descent(974/999): loss=0.48343235374374743\n",
      "Gradient Descent(975/999): loss=0.4834320410336364\n",
      "Gradient Descent(976/999): loss=0.48343172832376324\n",
      "Gradient Descent(977/999): loss=0.48343141561412795\n",
      "Gradient Descent(978/999): loss=0.4834311029047307\n",
      "Gradient Descent(979/999): loss=0.48343079019557117\n",
      "Gradient Descent(980/999): loss=0.48343047748664975\n",
      "Gradient Descent(981/999): loss=0.4834301647779662\n",
      "Gradient Descent(982/999): loss=0.48342985206952055\n",
      "Gradient Descent(983/999): loss=0.4834295393613128\n",
      "Gradient Descent(984/999): loss=0.4834292266533429\n",
      "Gradient Descent(985/999): loss=0.483428913945611\n",
      "Gradient Descent(986/999): loss=0.483428601238117\n",
      "Gradient Descent(987/999): loss=0.48342828853086095\n",
      "Gradient Descent(988/999): loss=0.4834279758238427\n",
      "Gradient Descent(989/999): loss=0.48342766311706237\n",
      "Gradient Descent(990/999): loss=0.4834273504105201\n",
      "Gradient Descent(991/999): loss=0.4834270377042157\n",
      "Gradient Descent(992/999): loss=0.48342672499814915\n",
      "Gradient Descent(993/999): loss=0.4834264122923205\n",
      "Gradient Descent(994/999): loss=0.4834260995867297\n",
      "Gradient Descent(995/999): loss=0.483425786881377\n",
      "Gradient Descent(996/999): loss=0.4834254741762621\n",
      "Gradient Descent(997/999): loss=0.48342516147138515\n",
      "Gradient Descent(998/999): loss=0.4834248487667461\n",
      "Gradient Descent(999/999): loss=0.4834245360623449\n",
      "Gradient Descent: execution time=0.486 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "weights, loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5000000000000001\n",
      "Gradient Descent(1/999): loss=0.49823370920696813\n",
      "Gradient Descent(2/999): loss=0.49665918418101845\n",
      "Gradient Descent(3/999): loss=0.4952556012695155\n",
      "Gradient Descent(4/999): loss=0.4940043980394159\n",
      "Gradient Descent(5/999): loss=0.4928890277336968\n",
      "Gradient Descent(6/999): loss=0.49189474039111464\n",
      "Gradient Descent(7/999): loss=0.49100838773395034\n",
      "Gradient Descent(8/999): loss=0.490218249242799\n",
      "Gradient Descent(9/999): loss=0.48951387711772365\n",
      "Gradient Descent(10/999): loss=0.48888595807492313\n",
      "Gradient Descent(11/999): loss=0.4883261901507599\n",
      "Gradient Descent(12/999): loss=0.4878271728835165\n",
      "Gradient Descent(13/999): loss=0.4873823094202044\n",
      "Gradient Descent(14/999): loss=0.4869857192534963\n",
      "Gradient Descent(15/999): loss=0.486632160434469\n",
      "Gradient Descent(16/999): loss=0.4863169602321843\n",
      "Gradient Descent(17/999): loss=0.4860359533228785\n",
      "Gradient Descent(18/999): loss=0.48578542669112584\n",
      "Gradient Descent(19/999): loss=0.48556207051412775\n",
      "Gradient Descent(20/999): loss=0.48536293437942996\n",
      "Gradient Descent(21/999): loss=0.4851853882569128\n",
      "Gradient Descent(22/999): loss=0.48502708770879455\n",
      "Gradient Descent(23/999): loss=0.48488594287744435\n",
      "Gradient Descent(24/999): loss=0.48476009084078\n",
      "Gradient Descent(25/999): loss=0.4846478709695625\n",
      "Gradient Descent(26/999): loss=0.48454780296061983\n",
      "Gradient Descent(27/999): loss=0.48445856725541925\n",
      "Gradient Descent(28/999): loss=0.48437898758496795\n",
      "Gradient Descent(29/999): loss=0.48430801541014545\n",
      "Gradient Descent(30/999): loss=0.4842447160516449\n",
      "Gradient Descent(31/999): loss=0.484188256326049\n",
      "Gradient Descent(32/999): loss=0.4841378935244944\n",
      "Gradient Descent(33/999): loss=0.4840929655881289\n",
      "Gradient Descent(34/999): loss=0.4840528823504087\n",
      "Gradient Descent(35/999): loss=0.48401711773038414\n",
      "Gradient Descent(36/999): loss=0.4839852027737089\n",
      "Gradient Descent(37/999): loss=0.48395671944932\n",
      "Gradient Descent(38/999): loss=0.48393129511972877\n",
      "Gradient Descent(39/999): loss=0.48390859761177846\n",
      "Gradient Descent(40/999): loss=0.4838883308226631\n",
      "Gradient Descent(41/999): loss=0.48387023080308345\n",
      "Gradient Descent(42/999): loss=0.483854062265731\n",
      "Gradient Descent(43/999): loss=0.4838396154729102\n",
      "Gradient Descent(44/999): loss=0.4838267034621318\n",
      "Gradient Descent(45/999): loss=0.4838151595729765\n",
      "Gradient Descent(46/999): loss=0.4838048352425124\n",
      "Gradient Descent(47/999): loss=0.4837955980401084\n",
      "Gradient Descent(48/999): loss=0.4837873299156425\n",
      "Gradient Descent(49/999): loss=0.4837799256379375\n",
      "Gradient Descent(50/999): loss=0.48377329140276426\n",
      "Gradient Descent(51/999): loss=0.4837673435920011\n",
      "Gradient Descent(52/999): loss=0.48376200766753436\n",
      "Gradient Descent(53/999): loss=0.48375721718526954\n",
      "Gradient Descent(54/999): loss=0.48375291291621\n",
      "Gradient Descent(55/999): loss=0.48374904206297625\n",
      "Gradient Descent(56/999): loss=0.4837455575614035\n",
      "Gradient Descent(57/999): loss=0.4837424174579777\n",
      "Gradient Descent(58/999): loss=0.4837395843548746\n",
      "Gradient Descent(59/999): loss=0.4837370249152624\n",
      "Gradient Descent(60/999): loss=0.48373470942232244\n",
      "Gradient Descent(61/999): loss=0.48373261138615564\n",
      "Gradient Descent(62/999): loss=0.4837307071933746\n",
      "Gradient Descent(63/999): loss=0.48372897579474594\n",
      "Gradient Descent(64/999): loss=0.48372739842675155\n",
      "Gradient Descent(65/999): loss=0.48372595836338494\n",
      "Gradient Descent(66/999): loss=0.4837246406948998\n",
      "Gradient Descent(67/999): loss=0.4837234321305843\n",
      "Gradient Descent(68/999): loss=0.48372232082295197\n",
      "Gradient Descent(69/999): loss=0.48372129621102256\n",
      "Gradient Descent(70/999): loss=0.48372034888062243\n",
      "Gradient Descent(71/999): loss=0.48371947043985364\n",
      "Gradient Descent(72/999): loss=0.4837186534080858\n",
      "Gradient Descent(73/999): loss=0.4837178911170029\n",
      "Gradient Descent(74/999): loss=0.4837171776223946\n",
      "Gradient Descent(75/999): loss=0.48371650762552637\n",
      "Gradient Descent(76/999): loss=0.48371587640304814\n",
      "Gradient Descent(77/999): loss=0.48371527974451345\n",
      "Gradient Descent(78/999): loss=0.48371471389668425\n",
      "Gradient Descent(79/999): loss=0.4837141755138827\n",
      "Gradient Descent(80/999): loss=0.48371366161373464\n",
      "Gradient Descent(81/999): loss=0.4837131695377192\n",
      "Gradient Descent(82/999): loss=0.48371269691600155\n",
      "Gradient Descent(83/999): loss=0.48371224163608484\n",
      "Gradient Descent(84/999): loss=0.48371180181486606\n",
      "Gradient Descent(85/999): loss=0.4837113757737266\n",
      "Gradient Descent(86/999): loss=0.4837109620163272\n",
      "Gradient Descent(87/999): loss=0.4837105592088147\n",
      "Gradient Descent(88/999): loss=0.48371016616217777\n",
      "Gradient Descent(89/999): loss=0.48370978181651847\n",
      "Gradient Descent(90/999): loss=0.4837094052270322\n",
      "Gradient Descent(91/999): loss=0.4837090355515098\n",
      "Gradient Descent(92/999): loss=0.48370867203919676\n",
      "Gradient Descent(93/999): loss=0.48370831402086206\n",
      "Gradient Descent(94/999): loss=0.483707960899946\n",
      "Gradient Descent(95/999): loss=0.48370761214466873\n",
      "Gradient Descent(96/999): loss=0.4837072672809956\n",
      "Gradient Descent(97/999): loss=0.4837069258863673\n",
      "Gradient Descent(98/999): loss=0.4837065875841094\n",
      "Gradient Descent(99/999): loss=0.4837062520384502\n",
      "Gradient Descent(100/999): loss=0.48370591895007964\n",
      "Gradient Descent(101/999): loss=0.4837055880521888\n",
      "Gradient Descent(102/999): loss=0.4837052591069415\n",
      "Gradient Descent(103/999): loss=0.4837049319023281\n",
      "Gradient Descent(104/999): loss=0.4837046062493608\n",
      "Gradient Descent(105/999): loss=0.48370428197957355\n",
      "Gradient Descent(106/999): loss=0.4837039589427943\n",
      "Gradient Descent(107/999): loss=0.4837036370051576\n",
      "Gradient Descent(108/999): loss=0.48370331604733474\n",
      "Gradient Descent(109/999): loss=0.48370299596295424\n",
      "Gradient Descent(110/999): loss=0.48370267665719613\n",
      "Gradient Descent(111/999): loss=0.48370235804553613\n",
      "Gradient Descent(112/999): loss=0.4837020400526288\n",
      "Gradient Descent(113/999): loss=0.4837017226113103\n",
      "Gradient Descent(114/999): loss=0.48370140566170977\n",
      "Gradient Descent(115/999): loss=0.483701089150458\n",
      "Gradient Descent(116/999): loss=0.48370077302998127\n",
      "Gradient Descent(117/999): loss=0.4837004572578712\n",
      "Gradient Descent(118/999): loss=0.4837001417963251\n",
      "Gradient Descent(119/999): loss=0.4836998266116448\n",
      "Gradient Descent(120/999): loss=0.4836995116737916\n",
      "Gradient Descent(121/999): loss=0.48369919695598873\n",
      "Gradient Descent(122/999): loss=0.483698882434367\n",
      "Gradient Descent(123/999): loss=0.48369856808764916\n",
      "Gradient Descent(124/999): loss=0.4836982538968683\n",
      "Gradient Descent(125/999): loss=0.48369793984511733\n",
      "Gradient Descent(126/999): loss=0.4836976259173249\n",
      "Gradient Descent(127/999): loss=0.48369731210005645\n",
      "Gradient Descent(128/999): loss=0.4836969983813361\n",
      "Gradient Descent(129/999): loss=0.4836966847504884\n",
      "Gradient Descent(130/999): loss=0.48369637119799735\n",
      "Gradient Descent(131/999): loss=0.4836960577153799\n",
      "Gradient Descent(132/999): loss=0.48369574429507456\n",
      "Gradient Descent(133/999): loss=0.48369543093034073\n",
      "Gradient Descent(134/999): loss=0.48369511761516976\n",
      "Gradient Descent(135/999): loss=0.48369480434420564\n",
      "Gradient Descent(136/999): loss=0.4836944911126739\n",
      "Gradient Descent(137/999): loss=0.48369417791631814\n",
      "Gradient Descent(138/999): loss=0.4836938647513447\n",
      "Gradient Descent(139/999): loss=0.4836935516143718\n",
      "Gradient Descent(140/999): loss=0.48369323850238444\n",
      "Gradient Descent(141/999): loss=0.48369292541269543\n",
      "Gradient Descent(142/999): loss=0.48369261234290944\n",
      "Gradient Descent(143/999): loss=0.48369229929089086\n",
      "Gradient Descent(144/999): loss=0.48369198625473625\n",
      "Gradient Descent(145/999): loss=0.48369167323274886\n",
      "Gradient Descent(146/999): loss=0.4836913602234161\n",
      "Gradient Descent(147/999): loss=0.4836910472253896\n",
      "Gradient Descent(148/999): loss=0.48369073423746767\n",
      "Gradient Descent(149/999): loss=0.48369042125857875\n",
      "Gradient Descent(150/999): loss=0.4836901082877677\n",
      "Gradient Descent(151/999): loss=0.48368979532418355\n",
      "Gradient Descent(152/999): loss=0.4836894823670671\n",
      "Gradient Descent(153/999): loss=0.4836891694157419\n",
      "Gradient Descent(154/999): loss=0.4836888564696052\n",
      "Gradient Descent(155/999): loss=0.483688543528119\n",
      "Gradient Descent(156/999): loss=0.4836882305908045\n",
      "Gradient Descent(157/999): loss=0.4836879176572345\n",
      "Gradient Descent(158/999): loss=0.48368760472702815\n",
      "Gradient Descent(159/999): loss=0.48368729179984604\n",
      "Gradient Descent(160/999): loss=0.48368697887538575\n",
      "Gradient Descent(161/999): loss=0.48368666595337734\n",
      "Gradient Descent(162/999): loss=0.4836863530335807\n",
      "Gradient Descent(163/999): loss=0.48368604011578126\n",
      "Gradient Descent(164/999): loss=0.4836857271997882\n",
      "Gradient Descent(165/999): loss=0.48368541428543105\n",
      "Gradient Descent(166/999): loss=0.48368510137255816\n",
      "Gradient Descent(167/999): loss=0.4836847884610341\n",
      "Gradient Descent(168/999): loss=0.48368447555073824\n",
      "Gradient Descent(169/999): loss=0.4836841626415632\n",
      "Gradient Descent(170/999): loss=0.483683849733413\n",
      "Gradient Descent(171/999): loss=0.4836835368262022\n",
      "Gradient Descent(172/999): loss=0.48368322391985474\n",
      "Gradient Descent(173/999): loss=0.4836829110143026\n",
      "Gradient Descent(174/999): loss=0.4836825981094853\n",
      "Gradient Descent(175/999): loss=0.48368228520534906\n",
      "Gradient Descent(176/999): loss=0.4836819723018454\n",
      "Gradient Descent(177/999): loss=0.4836816593989319\n",
      "Gradient Descent(178/999): loss=0.48368134649657013\n",
      "Gradient Descent(179/999): loss=0.483681033594726\n",
      "Gradient Descent(180/999): loss=0.48368072069336915\n",
      "Gradient Descent(181/999): loss=0.4836804077924727\n",
      "Gradient Descent(182/999): loss=0.48368009489201225\n",
      "Gradient Descent(183/999): loss=0.48367978199196643\n",
      "Gradient Descent(184/999): loss=0.48367946909231607\n",
      "Gradient Descent(185/999): loss=0.483679156193044\n",
      "Gradient Descent(186/999): loss=0.4836788432941351\n",
      "Gradient Descent(187/999): loss=0.4836785303955758\n",
      "Gradient Descent(188/999): loss=0.48367821749735385\n",
      "Gradient Descent(189/999): loss=0.48367790459945864\n",
      "Gradient Descent(190/999): loss=0.4836775917018803\n",
      "Gradient Descent(191/999): loss=0.4836772788046104\n",
      "Gradient Descent(192/999): loss=0.4836769659076414\n",
      "Gradient Descent(193/999): loss=0.4836766530109665\n",
      "Gradient Descent(194/999): loss=0.4836763401145794\n",
      "Gradient Descent(195/999): loss=0.4836760272184747\n",
      "Gradient Descent(196/999): loss=0.48367571432264783\n",
      "Gradient Descent(197/999): loss=0.4836754014270942\n",
      "Gradient Descent(198/999): loss=0.48367508853181024\n",
      "Gradient Descent(199/999): loss=0.4836747756367924\n",
      "Gradient Descent(200/999): loss=0.48367446274203746\n",
      "Gradient Descent(201/999): loss=0.48367414984754303\n",
      "Gradient Descent(202/999): loss=0.48367383695330646\n",
      "Gradient Descent(203/999): loss=0.4836735240593257\n",
      "Gradient Descent(204/999): loss=0.48367321116559886\n",
      "Gradient Descent(205/999): loss=0.483672898272124\n",
      "Gradient Descent(206/999): loss=0.48367258537889984\n",
      "Gradient Descent(207/999): loss=0.4836722724859248\n",
      "Gradient Descent(208/999): loss=0.483671959593198\n",
      "Gradient Descent(209/999): loss=0.48367164670071805\n",
      "Gradient Descent(210/999): loss=0.4836713338084841\n",
      "Gradient Descent(211/999): loss=0.4836710209164951\n",
      "Gradient Descent(212/999): loss=0.48367070802475065\n",
      "Gradient Descent(213/999): loss=0.48367039513324983\n",
      "Gradient Descent(214/999): loss=0.483670082241992\n",
      "Gradient Descent(215/999): loss=0.4836697693509767\n",
      "Gradient Descent(216/999): loss=0.4836694564602035\n",
      "Gradient Descent(217/999): loss=0.48366914356967183\n",
      "Gradient Descent(218/999): loss=0.48366883067938143\n",
      "Gradient Descent(219/999): loss=0.4836685177893318\n",
      "Gradient Descent(220/999): loss=0.4836682048995228\n",
      "Gradient Descent(221/999): loss=0.4836678920099541\n",
      "Gradient Descent(222/999): loss=0.4836675791206254\n",
      "Gradient Descent(223/999): loss=0.48366726623153655\n",
      "Gradient Descent(224/999): loss=0.48366695334268733\n",
      "Gradient Descent(225/999): loss=0.48366664045407753\n",
      "Gradient Descent(226/999): loss=0.48366632756570704\n",
      "Gradient Descent(227/999): loss=0.4836660146775758\n",
      "Gradient Descent(228/999): loss=0.4836657017896836\n",
      "Gradient Descent(229/999): loss=0.48366538890203037\n",
      "Gradient Descent(230/999): loss=0.483665076014616\n",
      "Gradient Descent(231/999): loss=0.4836647631274403\n",
      "Gradient Descent(232/999): loss=0.48366445024050325\n",
      "Gradient Descent(233/999): loss=0.4836641373538049\n",
      "Gradient Descent(234/999): loss=0.4836638244673451\n",
      "Gradient Descent(235/999): loss=0.48366351158112375\n",
      "Gradient Descent(236/999): loss=0.4836631986951409\n",
      "Gradient Descent(237/999): loss=0.48366288580939637\n",
      "Gradient Descent(238/999): loss=0.4836625729238902\n",
      "Gradient Descent(239/999): loss=0.4836622600386225\n",
      "Gradient Descent(240/999): loss=0.48366194715359295\n",
      "Gradient Descent(241/999): loss=0.48366163426880177\n",
      "Gradient Descent(242/999): loss=0.48366132138424883\n",
      "Gradient Descent(243/999): loss=0.4836610084999341\n",
      "Gradient Descent(244/999): loss=0.4836606956158576\n",
      "Gradient Descent(245/999): loss=0.4836603827320192\n",
      "Gradient Descent(246/999): loss=0.4836600698484191\n",
      "Gradient Descent(247/999): loss=0.4836597569650571\n",
      "Gradient Descent(248/999): loss=0.4836594440819332\n",
      "Gradient Descent(249/999): loss=0.4836591311990475\n",
      "Gradient Descent(250/999): loss=0.4836588183163999\n",
      "Gradient Descent(251/999): loss=0.48365850543399036\n",
      "Gradient Descent(252/999): loss=0.483658192551819\n",
      "Gradient Descent(253/999): loss=0.4836578796698858\n",
      "Gradient Descent(254/999): loss=0.4836575667881906\n",
      "Gradient Descent(255/999): loss=0.4836572539067336\n",
      "Gradient Descent(256/999): loss=0.4836569410255146\n",
      "Gradient Descent(257/999): loss=0.48365662814453364\n",
      "Gradient Descent(258/999): loss=0.4836563152637908\n",
      "Gradient Descent(259/999): loss=0.48365600238328604\n",
      "Gradient Descent(260/999): loss=0.4836556895030193\n",
      "Gradient Descent(261/999): loss=0.4836553766229907\n",
      "Gradient Descent(262/999): loss=0.4836550637432001\n",
      "Gradient Descent(263/999): loss=0.4836547508636477\n",
      "Gradient Descent(264/999): loss=0.4836544379843333\n",
      "Gradient Descent(265/999): loss=0.48365412510525685\n",
      "Gradient Descent(266/999): loss=0.48365381222641846\n",
      "Gradient Descent(267/999): loss=0.48365349934781815\n",
      "Gradient Descent(268/999): loss=0.483653186469456\n",
      "Gradient Descent(269/999): loss=0.4836528735913318\n",
      "Gradient Descent(270/999): loss=0.48365256071344576\n",
      "Gradient Descent(271/999): loss=0.4836522478357977\n",
      "Gradient Descent(272/999): loss=0.4836519349583876\n",
      "Gradient Descent(273/999): loss=0.48365162208121565\n",
      "Gradient Descent(274/999): loss=0.48365130920428173\n",
      "Gradient Descent(275/999): loss=0.48365099632758574\n",
      "Gradient Descent(276/999): loss=0.48365068345112805\n",
      "Gradient Descent(277/999): loss=0.4836503705749083\n",
      "Gradient Descent(278/999): loss=0.4836500576989265\n",
      "Gradient Descent(279/999): loss=0.48364974482318285\n",
      "Gradient Descent(280/999): loss=0.4836494319476771\n",
      "Gradient Descent(281/999): loss=0.4836491190724095\n",
      "Gradient Descent(282/999): loss=0.48364880619738\n",
      "Gradient Descent(283/999): loss=0.48364849332258836\n",
      "Gradient Descent(284/999): loss=0.48364818044803487\n",
      "Gradient Descent(285/999): loss=0.48364786757371947\n",
      "Gradient Descent(286/999): loss=0.4836475546996421\n",
      "Gradient Descent(287/999): loss=0.48364724182580265\n",
      "Gradient Descent(288/999): loss=0.4836469289522014\n",
      "Gradient Descent(289/999): loss=0.4836466160788381\n",
      "Gradient Descent(290/999): loss=0.4836463032057129\n",
      "Gradient Descent(291/999): loss=0.48364599033282557\n",
      "Gradient Descent(292/999): loss=0.48364567746017645\n",
      "Gradient Descent(293/999): loss=0.48364536458776536\n",
      "Gradient Descent(294/999): loss=0.48364505171559224\n",
      "Gradient Descent(295/999): loss=0.48364473884365716\n",
      "Gradient Descent(296/999): loss=0.4836444259719602\n",
      "Gradient Descent(297/999): loss=0.4836441131005012\n",
      "Gradient Descent(298/999): loss=0.4836438002292803\n",
      "Gradient Descent(299/999): loss=0.4836434873582974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(300/999): loss=0.48364317448755256\n",
      "Gradient Descent(301/999): loss=0.48364286161704567\n",
      "Gradient Descent(302/999): loss=0.483642548746777\n",
      "Gradient Descent(303/999): loss=0.48364223587674615\n",
      "Gradient Descent(304/999): loss=0.48364192300695347\n",
      "Gradient Descent(305/999): loss=0.48364161013739865\n",
      "Gradient Descent(306/999): loss=0.483641297268082\n",
      "Gradient Descent(307/999): loss=0.48364098439900355\n",
      "Gradient Descent(308/999): loss=0.48364067153016294\n",
      "Gradient Descent(309/999): loss=0.48364035866156035\n",
      "Gradient Descent(310/999): loss=0.4836400457931959\n",
      "Gradient Descent(311/999): loss=0.48363973292506945\n",
      "Gradient Descent(312/999): loss=0.48363942005718097\n",
      "Gradient Descent(313/999): loss=0.4836391071895307\n",
      "Gradient Descent(314/999): loss=0.48363879432211826\n",
      "Gradient Descent(315/999): loss=0.4836384814549439\n",
      "Gradient Descent(316/999): loss=0.4836381685880076\n",
      "Gradient Descent(317/999): loss=0.48363785572130946\n",
      "Gradient Descent(318/999): loss=0.4836375428548492\n",
      "Gradient Descent(319/999): loss=0.483637229988627\n",
      "Gradient Descent(320/999): loss=0.4836369171226428\n",
      "Gradient Descent(321/999): loss=0.4836366042568967\n",
      "Gradient Descent(322/999): loss=0.48363629139138864\n",
      "Gradient Descent(323/999): loss=0.48363597852611856\n",
      "Gradient Descent(324/999): loss=0.48363566566108657\n",
      "Gradient Descent(325/999): loss=0.4836353527962926\n",
      "Gradient Descent(326/999): loss=0.48363503993173673\n",
      "Gradient Descent(327/999): loss=0.48363472706741867\n",
      "Gradient Descent(328/999): loss=0.48363441420333886\n",
      "Gradient Descent(329/999): loss=0.4836341013394969\n",
      "Gradient Descent(330/999): loss=0.4836337884758932\n",
      "Gradient Descent(331/999): loss=0.4836334756125273\n",
      "Gradient Descent(332/999): loss=0.4836331627493996\n",
      "Gradient Descent(333/999): loss=0.48363284988650984\n",
      "Gradient Descent(334/999): loss=0.4836325370238581\n",
      "Gradient Descent(335/999): loss=0.48363222416144447\n",
      "Gradient Descent(336/999): loss=0.4836319112992689\n",
      "Gradient Descent(337/999): loss=0.48363159843733117\n",
      "Gradient Descent(338/999): loss=0.4836312855756317\n",
      "Gradient Descent(339/999): loss=0.4836309727141701\n",
      "Gradient Descent(340/999): loss=0.4836306598529466\n",
      "Gradient Descent(341/999): loss=0.48363034699196106\n",
      "Gradient Descent(342/999): loss=0.4836300341312136\n",
      "Gradient Descent(343/999): loss=0.48362972127070425\n",
      "Gradient Descent(344/999): loss=0.48362940841043284\n",
      "Gradient Descent(345/999): loss=0.4836290955503994\n",
      "Gradient Descent(346/999): loss=0.4836287826906041\n",
      "Gradient Descent(347/999): loss=0.48362846983104685\n",
      "Gradient Descent(348/999): loss=0.48362815697172756\n",
      "Gradient Descent(349/999): loss=0.4836278441126462\n",
      "Gradient Descent(350/999): loss=0.483627531253803\n",
      "Gradient Descent(351/999): loss=0.4836272183951978\n",
      "Gradient Descent(352/999): loss=0.4836269055368307\n",
      "Gradient Descent(353/999): loss=0.4836265926787015\n",
      "Gradient Descent(354/999): loss=0.4836262798208104\n",
      "Gradient Descent(355/999): loss=0.48362596696315724\n",
      "Gradient Descent(356/999): loss=0.48362565410574226\n",
      "Gradient Descent(357/999): loss=0.4836253412485652\n",
      "Gradient Descent(358/999): loss=0.4836250283916262\n",
      "Gradient Descent(359/999): loss=0.48362471553492514\n",
      "Gradient Descent(360/999): loss=0.48362440267846224\n",
      "Gradient Descent(361/999): loss=0.48362408982223726\n",
      "Gradient Descent(362/999): loss=0.48362377696625036\n",
      "Gradient Descent(363/999): loss=0.4836234641105015\n",
      "Gradient Descent(364/999): loss=0.4836231512549906\n",
      "Gradient Descent(365/999): loss=0.48362283839971776\n",
      "Gradient Descent(366/999): loss=0.48362252554468305\n",
      "Gradient Descent(367/999): loss=0.48362221268988625\n",
      "Gradient Descent(368/999): loss=0.48362189983532744\n",
      "Gradient Descent(369/999): loss=0.48362158698100677\n",
      "Gradient Descent(370/999): loss=0.4836212741269241\n",
      "Gradient Descent(371/999): loss=0.4836209612730794\n",
      "Gradient Descent(372/999): loss=0.4836206484194728\n",
      "Gradient Descent(373/999): loss=0.48362033556610406\n",
      "Gradient Descent(374/999): loss=0.4836200227129735\n",
      "Gradient Descent(375/999): loss=0.4836197098600809\n",
      "Gradient Descent(376/999): loss=0.48361939700742623\n",
      "Gradient Descent(377/999): loss=0.4836190841550098\n",
      "Gradient Descent(378/999): loss=0.48361877130283126\n",
      "Gradient Descent(379/999): loss=0.48361845845089085\n",
      "Gradient Descent(380/999): loss=0.4836181455991883\n",
      "Gradient Descent(381/999): loss=0.48361783274772385\n",
      "Gradient Descent(382/999): loss=0.4836175198964975\n",
      "Gradient Descent(383/999): loss=0.483617207045509\n",
      "Gradient Descent(384/999): loss=0.4836168941947586\n",
      "Gradient Descent(385/999): loss=0.48361658134424634\n",
      "Gradient Descent(386/999): loss=0.483616268493972\n",
      "Gradient Descent(387/999): loss=0.48361595564393567\n",
      "Gradient Descent(388/999): loss=0.4836156427941373\n",
      "Gradient Descent(389/999): loss=0.483615329944577\n",
      "Gradient Descent(390/999): loss=0.48361501709525484\n",
      "Gradient Descent(391/999): loss=0.48361470424617053\n",
      "Gradient Descent(392/999): loss=0.4836143913973244\n",
      "Gradient Descent(393/999): loss=0.48361407854871613\n",
      "Gradient Descent(394/999): loss=0.483613765700346\n",
      "Gradient Descent(395/999): loss=0.48361345285221385\n",
      "Gradient Descent(396/999): loss=0.48361314000431976\n",
      "Gradient Descent(397/999): loss=0.48361282715666365\n",
      "Gradient Descent(398/999): loss=0.48361251430924557\n",
      "Gradient Descent(399/999): loss=0.4836122014620654\n",
      "Gradient Descent(400/999): loss=0.4836118886151233\n",
      "Gradient Descent(401/999): loss=0.48361157576841934\n",
      "Gradient Descent(402/999): loss=0.4836112629219534\n",
      "Gradient Descent(403/999): loss=0.4836109500757254\n",
      "Gradient Descent(404/999): loss=0.4836106372297354\n",
      "Gradient Descent(405/999): loss=0.48361032438398344\n",
      "Gradient Descent(406/999): loss=0.48361001153846944\n",
      "Gradient Descent(407/999): loss=0.48360969869319353\n",
      "Gradient Descent(408/999): loss=0.48360938584815566\n",
      "Gradient Descent(409/999): loss=0.48360907300335576\n",
      "Gradient Descent(410/999): loss=0.48360876015879384\n",
      "Gradient Descent(411/999): loss=0.48360844731447006\n",
      "Gradient Descent(412/999): loss=0.4836081344703842\n",
      "Gradient Descent(413/999): loss=0.4836078216265364\n",
      "Gradient Descent(414/999): loss=0.4836075087829266\n",
      "Gradient Descent(415/999): loss=0.48360719593955476\n",
      "Gradient Descent(416/999): loss=0.483606883096421\n",
      "Gradient Descent(417/999): loss=0.4836065702535252\n",
      "Gradient Descent(418/999): loss=0.4836062574108675\n",
      "Gradient Descent(419/999): loss=0.48360594456844774\n",
      "Gradient Descent(420/999): loss=0.4836056317262661\n",
      "Gradient Descent(421/999): loss=0.4836053188843224\n",
      "Gradient Descent(422/999): loss=0.4836050060426167\n",
      "Gradient Descent(423/999): loss=0.48360469320114907\n",
      "Gradient Descent(424/999): loss=0.4836043803599194\n",
      "Gradient Descent(425/999): loss=0.4836040675189277\n",
      "Gradient Descent(426/999): loss=0.4836037546781741\n",
      "Gradient Descent(427/999): loss=0.4836034418376586\n",
      "Gradient Descent(428/999): loss=0.4836031289973809\n",
      "Gradient Descent(429/999): loss=0.4836028161573414\n",
      "Gradient Descent(430/999): loss=0.4836025033175398\n",
      "Gradient Descent(431/999): loss=0.4836021904779762\n",
      "Gradient Descent(432/999): loss=0.4836018776386507\n",
      "Gradient Descent(433/999): loss=0.4836015647995631\n",
      "Gradient Descent(434/999): loss=0.4836012519607137\n",
      "Gradient Descent(435/999): loss=0.4836009391221021\n",
      "Gradient Descent(436/999): loss=0.48360062628372863\n",
      "Gradient Descent(437/999): loss=0.4836003134455932\n",
      "Gradient Descent(438/999): loss=0.4836000006076957\n",
      "Gradient Descent(439/999): loss=0.48359968777003626\n",
      "Gradient Descent(440/999): loss=0.4835993749326148\n",
      "Gradient Descent(441/999): loss=0.4835990620954314\n",
      "Gradient Descent(442/999): loss=0.483598749258486\n",
      "Gradient Descent(443/999): loss=0.4835984364217786\n",
      "Gradient Descent(444/999): loss=0.48359812358530924\n",
      "Gradient Descent(445/999): loss=0.48359781074907776\n",
      "Gradient Descent(446/999): loss=0.48359749791308443\n",
      "Gradient Descent(447/999): loss=0.4835971850773291\n",
      "Gradient Descent(448/999): loss=0.4835968722418117\n",
      "Gradient Descent(449/999): loss=0.4835965594065324\n",
      "Gradient Descent(450/999): loss=0.4835962465714911\n",
      "Gradient Descent(451/999): loss=0.48359593373668774\n",
      "Gradient Descent(452/999): loss=0.4835956209021225\n",
      "Gradient Descent(453/999): loss=0.4835953080677951\n",
      "Gradient Descent(454/999): loss=0.48359499523370575\n",
      "Gradient Descent(455/999): loss=0.48359468239985454\n",
      "Gradient Descent(456/999): loss=0.4835943695662413\n",
      "Gradient Descent(457/999): loss=0.4835940567328661\n",
      "Gradient Descent(458/999): loss=0.4835937438997287\n",
      "Gradient Descent(459/999): loss=0.48359343106682956\n",
      "Gradient Descent(460/999): loss=0.4835931182341683\n",
      "Gradient Descent(461/999): loss=0.48359280540174504\n",
      "Gradient Descent(462/999): loss=0.4835924925695599\n",
      "Gradient Descent(463/999): loss=0.4835921797376126\n",
      "Gradient Descent(464/999): loss=0.4835918669059034\n",
      "Gradient Descent(465/999): loss=0.4835915540744321\n",
      "Gradient Descent(466/999): loss=0.48359124124319897\n",
      "Gradient Descent(467/999): loss=0.4835909284122038\n",
      "Gradient Descent(468/999): loss=0.48359061558144667\n",
      "Gradient Descent(469/999): loss=0.48359030275092757\n",
      "Gradient Descent(470/999): loss=0.4835899899206463\n",
      "Gradient Descent(471/999): loss=0.4835896770906032\n",
      "Gradient Descent(472/999): loss=0.483589364260798\n",
      "Gradient Descent(473/999): loss=0.4835890514312308\n",
      "Gradient Descent(474/999): loss=0.4835887386019017\n",
      "Gradient Descent(475/999): loss=0.4835884257728106\n",
      "Gradient Descent(476/999): loss=0.48358811294395737\n",
      "Gradient Descent(477/999): loss=0.48358780011534225\n",
      "Gradient Descent(478/999): loss=0.4835874872869652\n",
      "Gradient Descent(479/999): loss=0.48358717445882604\n",
      "Gradient Descent(480/999): loss=0.4835868616309249\n",
      "Gradient Descent(481/999): loss=0.48358654880326185\n",
      "Gradient Descent(482/999): loss=0.4835862359758367\n",
      "Gradient Descent(483/999): loss=0.4835859231486496\n",
      "Gradient Descent(484/999): loss=0.48358561032170055\n",
      "Gradient Descent(485/999): loss=0.4835852974949894\n",
      "Gradient Descent(486/999): loss=0.48358498466851635\n",
      "Gradient Descent(487/999): loss=0.4835846718422813\n",
      "Gradient Descent(488/999): loss=0.48358435901628416\n",
      "Gradient Descent(489/999): loss=0.4835840461905251\n",
      "Gradient Descent(490/999): loss=0.483583733365004\n",
      "Gradient Descent(491/999): loss=0.4835834205397209\n",
      "Gradient Descent(492/999): loss=0.4835831077146759\n",
      "Gradient Descent(493/999): loss=0.48358279488986883\n",
      "Gradient Descent(494/999): loss=0.4835824820652997\n",
      "Gradient Descent(495/999): loss=0.4835821692409687\n",
      "Gradient Descent(496/999): loss=0.4835818564168756\n",
      "Gradient Descent(497/999): loss=0.48358154359302064\n",
      "Gradient Descent(498/999): loss=0.48358123076940357\n",
      "Gradient Descent(499/999): loss=0.4835809179460244\n",
      "Gradient Descent(500/999): loss=0.48358060512288337\n",
      "Gradient Descent(501/999): loss=0.4835802922999803\n",
      "Gradient Descent(502/999): loss=0.4835799794773153\n",
      "Gradient Descent(503/999): loss=0.4835796666548882\n",
      "Gradient Descent(504/999): loss=0.4835793538326991\n",
      "Gradient Descent(505/999): loss=0.4835790410107481\n",
      "Gradient Descent(506/999): loss=0.4835787281890351\n",
      "Gradient Descent(507/999): loss=0.48357841536755997\n",
      "Gradient Descent(508/999): loss=0.48357810254632294\n",
      "Gradient Descent(509/999): loss=0.4835777897253239\n",
      "Gradient Descent(510/999): loss=0.4835774769045627\n",
      "Gradient Descent(511/999): loss=0.4835771640840398\n",
      "Gradient Descent(512/999): loss=0.4835768512637547\n",
      "Gradient Descent(513/999): loss=0.4835765384437076\n",
      "Gradient Descent(514/999): loss=0.4835762256238986\n",
      "Gradient Descent(515/999): loss=0.48357591280432743\n",
      "Gradient Descent(516/999): loss=0.4835755999849944\n",
      "Gradient Descent(517/999): loss=0.4835752871658993\n",
      "Gradient Descent(518/999): loss=0.48357497434704216\n",
      "Gradient Descent(519/999): loss=0.48357466152842316\n",
      "Gradient Descent(520/999): loss=0.4835743487100422\n",
      "Gradient Descent(521/999): loss=0.48357403589189896\n",
      "Gradient Descent(522/999): loss=0.483573723073994\n",
      "Gradient Descent(523/999): loss=0.4835734102563269\n",
      "Gradient Descent(524/999): loss=0.4835730974388978\n",
      "Gradient Descent(525/999): loss=0.4835727846217067\n",
      "Gradient Descent(526/999): loss=0.4835724718047535\n",
      "Gradient Descent(527/999): loss=0.4835721589880384\n",
      "Gradient Descent(528/999): loss=0.48357184617156135\n",
      "Gradient Descent(529/999): loss=0.4835715333553222\n",
      "Gradient Descent(530/999): loss=0.48357122053932106\n",
      "Gradient Descent(531/999): loss=0.48357090772355804\n",
      "Gradient Descent(532/999): loss=0.48357059490803295\n",
      "Gradient Descent(533/999): loss=0.4835702820927458\n",
      "Gradient Descent(534/999): loss=0.4835699692776966\n",
      "Gradient Descent(535/999): loss=0.4835696564628855\n",
      "Gradient Descent(536/999): loss=0.48356934364831233\n",
      "Gradient Descent(537/999): loss=0.4835690308339772\n",
      "Gradient Descent(538/999): loss=0.48356871801988\n",
      "Gradient Descent(539/999): loss=0.4835684052060209\n",
      "Gradient Descent(540/999): loss=0.48356809239239973\n",
      "Gradient Descent(541/999): loss=0.4835677795790165\n",
      "Gradient Descent(542/999): loss=0.48356746676587137\n",
      "Gradient Descent(543/999): loss=0.48356715395296423\n",
      "Gradient Descent(544/999): loss=0.48356684114029497\n",
      "Gradient Descent(545/999): loss=0.4835665283278638\n",
      "Gradient Descent(546/999): loss=0.4835662155156706\n",
      "Gradient Descent(547/999): loss=0.48356590270371536\n",
      "Gradient Descent(548/999): loss=0.4835655898919981\n",
      "Gradient Descent(549/999): loss=0.4835652770805189\n",
      "Gradient Descent(550/999): loss=0.4835649642692777\n",
      "Gradient Descent(551/999): loss=0.48356465145827443\n",
      "Gradient Descent(552/999): loss=0.48356433864750914\n",
      "Gradient Descent(553/999): loss=0.48356402583698194\n",
      "Gradient Descent(554/999): loss=0.4835637130266926\n",
      "Gradient Descent(555/999): loss=0.4835634002166413\n",
      "Gradient Descent(556/999): loss=0.4835630874068281\n",
      "Gradient Descent(557/999): loss=0.4835627745972527\n",
      "Gradient Descent(558/999): loss=0.48356246178791545\n",
      "Gradient Descent(559/999): loss=0.4835621489788161\n",
      "Gradient Descent(560/999): loss=0.4835618361699548\n",
      "Gradient Descent(561/999): loss=0.48356152336133146\n",
      "Gradient Descent(562/999): loss=0.48356121055294604\n",
      "Gradient Descent(563/999): loss=0.48356089774479877\n",
      "Gradient Descent(564/999): loss=0.48356058493688936\n",
      "Gradient Descent(565/999): loss=0.483560272129218\n",
      "Gradient Descent(566/999): loss=0.4835599593217846\n",
      "Gradient Descent(567/999): loss=0.48355964651458916\n",
      "Gradient Descent(568/999): loss=0.4835593337076317\n",
      "Gradient Descent(569/999): loss=0.4835590209009123\n",
      "Gradient Descent(570/999): loss=0.48355870809443086\n",
      "Gradient Descent(571/999): loss=0.48355839528818745\n",
      "Gradient Descent(572/999): loss=0.483558082482182\n",
      "Gradient Descent(573/999): loss=0.4835577696764145\n",
      "Gradient Descent(574/999): loss=0.483557456870885\n",
      "Gradient Descent(575/999): loss=0.48355714406559347\n",
      "Gradient Descent(576/999): loss=0.48355683126054\n",
      "Gradient Descent(577/999): loss=0.48355651845572445\n",
      "Gradient Descent(578/999): loss=0.48355620565114693\n",
      "Gradient Descent(579/999): loss=0.4835558928468074\n",
      "Gradient Descent(580/999): loss=0.48355558004270577\n",
      "Gradient Descent(581/999): loss=0.48355526723884223\n",
      "Gradient Descent(582/999): loss=0.48355495443521657\n",
      "Gradient Descent(583/999): loss=0.483554641631829\n",
      "Gradient Descent(584/999): loss=0.48355432882867944\n",
      "Gradient Descent(585/999): loss=0.48355401602576775\n",
      "Gradient Descent(586/999): loss=0.483553703223094\n",
      "Gradient Descent(587/999): loss=0.48355339042065837\n",
      "Gradient Descent(588/999): loss=0.4835530776184607\n",
      "Gradient Descent(589/999): loss=0.483552764816501\n",
      "Gradient Descent(590/999): loss=0.4835524520147793\n",
      "Gradient Descent(591/999): loss=0.4835521392132956\n",
      "Gradient Descent(592/999): loss=0.48355182641204986\n",
      "Gradient Descent(593/999): loss=0.4835515136110421\n",
      "Gradient Descent(594/999): loss=0.48355120081027236\n",
      "Gradient Descent(595/999): loss=0.48355088800974055\n",
      "Gradient Descent(596/999): loss=0.48355057520944666\n",
      "Gradient Descent(597/999): loss=0.48355026240939086\n",
      "Gradient Descent(598/999): loss=0.48354994960957304\n",
      "Gradient Descent(599/999): loss=0.4835496368099932\n",
      "Gradient Descent(600/999): loss=0.48354932401065126\n",
      "Gradient Descent(601/999): loss=0.4835490112115474\n",
      "Gradient Descent(602/999): loss=0.48354869841268144\n",
      "Gradient Descent(603/999): loss=0.4835483856140536\n",
      "Gradient Descent(604/999): loss=0.48354807281566364\n",
      "Gradient Descent(605/999): loss=0.4835477600175116\n",
      "Gradient Descent(606/999): loss=0.4835474472195977\n",
      "Gradient Descent(607/999): loss=0.4835471344219217\n",
      "Gradient Descent(608/999): loss=0.4835468216244836\n",
      "Gradient Descent(609/999): loss=0.48354650882728367\n",
      "Gradient Descent(610/999): loss=0.4835461960303215\n",
      "Gradient Descent(611/999): loss=0.4835458832335975\n",
      "Gradient Descent(612/999): loss=0.4835455704371114\n",
      "Gradient Descent(613/999): loss=0.4835452576408633\n",
      "Gradient Descent(614/999): loss=0.4835449448448531\n",
      "Gradient Descent(615/999): loss=0.48354463204908105\n",
      "Gradient Descent(616/999): loss=0.48354431925354674\n",
      "Gradient Descent(617/999): loss=0.48354400645825063\n",
      "Gradient Descent(618/999): loss=0.48354369366319244\n",
      "Gradient Descent(619/999): loss=0.4835433808683722\n",
      "Gradient Descent(620/999): loss=0.4835430680737899\n",
      "Gradient Descent(621/999): loss=0.48354275527944574\n",
      "Gradient Descent(622/999): loss=0.48354244248533934\n",
      "Gradient Descent(623/999): loss=0.48354212969147103\n",
      "Gradient Descent(624/999): loss=0.4835418168978407\n",
      "Gradient Descent(625/999): loss=0.4835415041044483\n",
      "Gradient Descent(626/999): loss=0.4835411913112939\n",
      "Gradient Descent(627/999): loss=0.48354087851837746\n",
      "Gradient Descent(628/999): loss=0.4835405657256991\n",
      "Gradient Descent(629/999): loss=0.4835402529332587\n",
      "Gradient Descent(630/999): loss=0.4835399401410561\n",
      "Gradient Descent(631/999): loss=0.4835396273490917\n",
      "Gradient Descent(632/999): loss=0.4835393145573652\n",
      "Gradient Descent(633/999): loss=0.4835390017658766\n",
      "Gradient Descent(634/999): loss=0.4835386889746261\n",
      "Gradient Descent(635/999): loss=0.4835383761836135\n",
      "Gradient Descent(636/999): loss=0.48353806339283883\n",
      "Gradient Descent(637/999): loss=0.4835377506023022\n",
      "Gradient Descent(638/999): loss=0.48353743781200353\n",
      "Gradient Descent(639/999): loss=0.48353712502194285\n",
      "Gradient Descent(640/999): loss=0.48353681223212014\n",
      "Gradient Descent(641/999): loss=0.4835364994425354\n",
      "Gradient Descent(642/999): loss=0.48353618665318865\n",
      "Gradient Descent(643/999): loss=0.48353587386407987\n",
      "Gradient Descent(644/999): loss=0.48353556107520906\n",
      "Gradient Descent(645/999): loss=0.4835352482865763\n",
      "Gradient Descent(646/999): loss=0.4835349354981814\n",
      "Gradient Descent(647/999): loss=0.48353462271002445\n",
      "Gradient Descent(648/999): loss=0.4835343099221056\n",
      "Gradient Descent(649/999): loss=0.4835339971344247\n",
      "Gradient Descent(650/999): loss=0.4835336843469817\n",
      "Gradient Descent(651/999): loss=0.4835333715597767\n",
      "Gradient Descent(652/999): loss=0.4835330587728096\n",
      "Gradient Descent(653/999): loss=0.4835327459860806\n",
      "Gradient Descent(654/999): loss=0.48353243319958955\n",
      "Gradient Descent(655/999): loss=0.48353212041333643\n",
      "Gradient Descent(656/999): loss=0.48353180762732134\n",
      "Gradient Descent(657/999): loss=0.4835314948415442\n",
      "Gradient Descent(658/999): loss=0.4835311820560049\n",
      "Gradient Descent(659/999): loss=0.4835308692707038\n",
      "Gradient Descent(660/999): loss=0.4835305564856405\n",
      "Gradient Descent(661/999): loss=0.4835302437008153\n",
      "Gradient Descent(662/999): loss=0.48352993091622803\n",
      "Gradient Descent(663/999): loss=0.48352961813187867\n",
      "Gradient Descent(664/999): loss=0.48352930534776734\n",
      "Gradient Descent(665/999): loss=0.4835289925638939\n",
      "Gradient Descent(666/999): loss=0.48352867978025854\n",
      "Gradient Descent(667/999): loss=0.483528366996861\n",
      "Gradient Descent(668/999): loss=0.48352805421370165\n",
      "Gradient Descent(669/999): loss=0.48352774143078014\n",
      "Gradient Descent(670/999): loss=0.4835274286480967\n",
      "Gradient Descent(671/999): loss=0.4835271158656511\n",
      "Gradient Descent(672/999): loss=0.48352680308344353\n",
      "Gradient Descent(673/999): loss=0.48352649030147393\n",
      "Gradient Descent(674/999): loss=0.4835261775197422\n",
      "Gradient Descent(675/999): loss=0.4835258647382486\n",
      "Gradient Descent(676/999): loss=0.48352555195699287\n",
      "Gradient Descent(677/999): loss=0.4835252391759752\n",
      "Gradient Descent(678/999): loss=0.48352492639519534\n",
      "Gradient Descent(679/999): loss=0.4835246136146535\n",
      "Gradient Descent(680/999): loss=0.48352430083434966\n",
      "Gradient Descent(681/999): loss=0.4835239880542838\n",
      "Gradient Descent(682/999): loss=0.48352367527445594\n",
      "Gradient Descent(683/999): loss=0.48352336249486605\n",
      "Gradient Descent(684/999): loss=0.483523049715514\n",
      "Gradient Descent(685/999): loss=0.4835227369364\n",
      "Gradient Descent(686/999): loss=0.48352242415752406\n",
      "Gradient Descent(687/999): loss=0.48352211137888607\n",
      "Gradient Descent(688/999): loss=0.4835217986004859\n",
      "Gradient Descent(689/999): loss=0.48352148582232374\n",
      "Gradient Descent(690/999): loss=0.4835211730443995\n",
      "Gradient Descent(691/999): loss=0.4835208602667135\n",
      "Gradient Descent(692/999): loss=0.4835205474892652\n",
      "Gradient Descent(693/999): loss=0.4835202347120549\n",
      "Gradient Descent(694/999): loss=0.4835199219350827\n",
      "Gradient Descent(695/999): loss=0.48351960915834835\n",
      "Gradient Descent(696/999): loss=0.48351929638185204\n",
      "Gradient Descent(697/999): loss=0.48351898360559353\n",
      "Gradient Descent(698/999): loss=0.48351867082957317\n",
      "Gradient Descent(699/999): loss=0.4835183580537907\n",
      "Gradient Descent(700/999): loss=0.48351804527824616\n",
      "Gradient Descent(701/999): loss=0.48351773250293956\n",
      "Gradient Descent(702/999): loss=0.48351741972787105\n",
      "Gradient Descent(703/999): loss=0.48351710695304034\n",
      "Gradient Descent(704/999): loss=0.4835167941784478\n",
      "Gradient Descent(705/999): loss=0.4835164814040931\n",
      "Gradient Descent(706/999): loss=0.4835161686299764\n",
      "Gradient Descent(707/999): loss=0.4835158558560976\n",
      "Gradient Descent(708/999): loss=0.48351554308245687\n",
      "Gradient Descent(709/999): loss=0.483515230309054\n",
      "Gradient Descent(710/999): loss=0.48351491753588904\n",
      "Gradient Descent(711/999): loss=0.48351460476296226\n",
      "Gradient Descent(712/999): loss=0.48351429199027324\n",
      "Gradient Descent(713/999): loss=0.48351397921782235\n",
      "Gradient Descent(714/999): loss=0.4835136664456093\n",
      "Gradient Descent(715/999): loss=0.48351335367363424\n",
      "Gradient Descent(716/999): loss=0.4835130409018972\n",
      "Gradient Descent(717/999): loss=0.48351272813039803\n",
      "Gradient Descent(718/999): loss=0.48351241535913675\n",
      "Gradient Descent(719/999): loss=0.4835121025881137\n",
      "Gradient Descent(720/999): loss=0.48351178981732845\n",
      "Gradient Descent(721/999): loss=0.48351147704678116\n",
      "Gradient Descent(722/999): loss=0.4835111642764717\n",
      "Gradient Descent(723/999): loss=0.48351085150640044\n",
      "Gradient Descent(724/999): loss=0.483510538736567\n",
      "Gradient Descent(725/999): loss=0.4835102259669716\n",
      "Gradient Descent(726/999): loss=0.48350991319761416\n",
      "Gradient Descent(727/999): loss=0.48350960042849467\n",
      "Gradient Descent(728/999): loss=0.483509287659613\n",
      "Gradient Descent(729/999): loss=0.4835089748909695\n",
      "Gradient Descent(730/999): loss=0.48350866212256377\n",
      "Gradient Descent(731/999): loss=0.48350834935439607\n",
      "Gradient Descent(732/999): loss=0.4835080365864663\n",
      "Gradient Descent(733/999): loss=0.4835077238187746\n",
      "Gradient Descent(734/999): loss=0.48350741105132083\n",
      "Gradient Descent(735/999): loss=0.48350709828410493\n",
      "Gradient Descent(736/999): loss=0.483506785517127\n",
      "Gradient Descent(737/999): loss=0.48350647275038716\n",
      "Gradient Descent(738/999): loss=0.48350615998388513\n",
      "Gradient Descent(739/999): loss=0.48350584721762113\n",
      "Gradient Descent(740/999): loss=0.48350553445159505\n",
      "Gradient Descent(741/999): loss=0.483505221685807\n",
      "Gradient Descent(742/999): loss=0.4835049089202568\n",
      "Gradient Descent(743/999): loss=0.48350459615494457\n",
      "Gradient Descent(744/999): loss=0.4835042833898704\n",
      "Gradient Descent(745/999): loss=0.48350397062503414\n",
      "Gradient Descent(746/999): loss=0.48350365786043586\n",
      "Gradient Descent(747/999): loss=0.48350334509607545\n",
      "Gradient Descent(748/999): loss=0.483503032331953\n",
      "Gradient Descent(749/999): loss=0.48350271956806856\n",
      "Gradient Descent(750/999): loss=0.483502406804422\n",
      "Gradient Descent(751/999): loss=0.4835020940410135\n",
      "Gradient Descent(752/999): loss=0.48350178127784293\n",
      "Gradient Descent(753/999): loss=0.48350146851491027\n",
      "Gradient Descent(754/999): loss=0.4835011557522157\n",
      "Gradient Descent(755/999): loss=0.48350084298975887\n",
      "Gradient Descent(756/999): loss=0.4835005302275402\n",
      "Gradient Descent(757/999): loss=0.4835002174655594\n",
      "Gradient Descent(758/999): loss=0.48349990470381654\n",
      "Gradient Descent(759/999): loss=0.4834995919423116\n",
      "Gradient Descent(760/999): loss=0.4834992791810447\n",
      "Gradient Descent(761/999): loss=0.48349896642001566\n",
      "Gradient Descent(762/999): loss=0.4834986536592246\n",
      "Gradient Descent(763/999): loss=0.48349834089867155\n",
      "Gradient Descent(764/999): loss=0.4834980281383564\n",
      "Gradient Descent(765/999): loss=0.4834977153782792\n",
      "Gradient Descent(766/999): loss=0.48349740261844\n",
      "Gradient Descent(767/999): loss=0.4834970898588387\n",
      "Gradient Descent(768/999): loss=0.48349677709947536\n",
      "Gradient Descent(769/999): loss=0.48349646434035004\n",
      "Gradient Descent(770/999): loss=0.48349615158146264\n",
      "Gradient Descent(771/999): loss=0.4834958388228131\n",
      "Gradient Descent(772/999): loss=0.48349552606440155\n",
      "Gradient Descent(773/999): loss=0.483495213306228\n",
      "Gradient Descent(774/999): loss=0.4834949005482924\n",
      "Gradient Descent(775/999): loss=0.4834945877905947\n",
      "Gradient Descent(776/999): loss=0.4834942750331351\n",
      "Gradient Descent(777/999): loss=0.48349396227591335\n",
      "Gradient Descent(778/999): loss=0.4834936495189295\n",
      "Gradient Descent(779/999): loss=0.48349333676218365\n",
      "Gradient Descent(780/999): loss=0.4834930240056758\n",
      "Gradient Descent(781/999): loss=0.4834927112494058\n",
      "Gradient Descent(782/999): loss=0.4834923984933737\n",
      "Gradient Descent(783/999): loss=0.4834920857375797\n",
      "Gradient Descent(784/999): loss=0.48349177298202356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(785/999): loss=0.48349146022670536\n",
      "Gradient Descent(786/999): loss=0.4834911474716252\n",
      "Gradient Descent(787/999): loss=0.48349083471678295\n",
      "Gradient Descent(788/999): loss=0.4834905219621786\n",
      "Gradient Descent(789/999): loss=0.4834902092078123\n",
      "Gradient Descent(790/999): loss=0.48348989645368384\n",
      "Gradient Descent(791/999): loss=0.4834895836997934\n",
      "Gradient Descent(792/999): loss=0.4834892709461408\n",
      "Gradient Descent(793/999): loss=0.4834889581927263\n",
      "Gradient Descent(794/999): loss=0.4834886454395496\n",
      "Gradient Descent(795/999): loss=0.4834883326866109\n",
      "Gradient Descent(796/999): loss=0.4834880199339103\n",
      "Gradient Descent(797/999): loss=0.4834877071814475\n",
      "Gradient Descent(798/999): loss=0.4834873944292226\n",
      "Gradient Descent(799/999): loss=0.4834870816772358\n",
      "Gradient Descent(800/999): loss=0.4834867689254868\n",
      "Gradient Descent(801/999): loss=0.4834864561739759\n",
      "Gradient Descent(802/999): loss=0.48348614342270285\n",
      "Gradient Descent(803/999): loss=0.48348583067166767\n",
      "Gradient Descent(804/999): loss=0.4834855179208706\n",
      "Gradient Descent(805/999): loss=0.4834852051703114\n",
      "Gradient Descent(806/999): loss=0.48348489241999004\n",
      "Gradient Descent(807/999): loss=0.4834845796699067\n",
      "Gradient Descent(808/999): loss=0.4834842669200613\n",
      "Gradient Descent(809/999): loss=0.4834839541704539\n",
      "Gradient Descent(810/999): loss=0.48348364142108446\n",
      "Gradient Descent(811/999): loss=0.4834833286719529\n",
      "Gradient Descent(812/999): loss=0.4834830159230593\n",
      "Gradient Descent(813/999): loss=0.48348270317440367\n",
      "Gradient Descent(814/999): loss=0.48348239042598595\n",
      "Gradient Descent(815/999): loss=0.4834820776778062\n",
      "Gradient Descent(816/999): loss=0.4834817649298644\n",
      "Gradient Descent(817/999): loss=0.4834814521821605\n",
      "Gradient Descent(818/999): loss=0.4834811394346946\n",
      "Gradient Descent(819/999): loss=0.48348082668746656\n",
      "Gradient Descent(820/999): loss=0.48348051394047653\n",
      "Gradient Descent(821/999): loss=0.4834802011937244\n",
      "Gradient Descent(822/999): loss=0.4834798884472103\n",
      "Gradient Descent(823/999): loss=0.48347957570093414\n",
      "Gradient Descent(824/999): loss=0.4834792629548958\n",
      "Gradient Descent(825/999): loss=0.4834789502090955\n",
      "Gradient Descent(826/999): loss=0.48347863746353303\n",
      "Gradient Descent(827/999): loss=0.4834783247182087\n",
      "Gradient Descent(828/999): loss=0.4834780119731221\n",
      "Gradient Descent(829/999): loss=0.48347769922827355\n",
      "Gradient Descent(830/999): loss=0.48347738648366295\n",
      "Gradient Descent(831/999): loss=0.4834770737392902\n",
      "Gradient Descent(832/999): loss=0.48347676099515546\n",
      "Gradient Descent(833/999): loss=0.48347644825125874\n",
      "Gradient Descent(834/999): loss=0.48347613550759977\n",
      "Gradient Descent(835/999): loss=0.4834758227641789\n",
      "Gradient Descent(836/999): loss=0.483475510020996\n",
      "Gradient Descent(837/999): loss=0.48347519727805083\n",
      "Gradient Descent(838/999): loss=0.48347488453534376\n",
      "Gradient Descent(839/999): loss=0.48347457179287456\n",
      "Gradient Descent(840/999): loss=0.4834742590506434\n",
      "Gradient Descent(841/999): loss=0.4834739463086501\n",
      "Gradient Descent(842/999): loss=0.48347363356689477\n",
      "Gradient Descent(843/999): loss=0.4834733208253773\n",
      "Gradient Descent(844/999): loss=0.48347300808409793\n",
      "Gradient Descent(845/999): loss=0.4834726953430564\n",
      "Gradient Descent(846/999): loss=0.48347238260225284\n",
      "Gradient Descent(847/999): loss=0.4834720698616871\n",
      "Gradient Descent(848/999): loss=0.4834717571213594\n",
      "Gradient Descent(849/999): loss=0.4834714443812696\n",
      "Gradient Descent(850/999): loss=0.48347113164141775\n",
      "Gradient Descent(851/999): loss=0.4834708189018039\n",
      "Gradient Descent(852/999): loss=0.483470506162428\n",
      "Gradient Descent(853/999): loss=0.48347019342328984\n",
      "Gradient Descent(854/999): loss=0.48346988068438984\n",
      "Gradient Descent(855/999): loss=0.4834695679457277\n",
      "Gradient Descent(856/999): loss=0.4834692552073035\n",
      "Gradient Descent(857/999): loss=0.48346894246911715\n",
      "Gradient Descent(858/999): loss=0.4834686297311688\n",
      "Gradient Descent(859/999): loss=0.4834683169934585\n",
      "Gradient Descent(860/999): loss=0.48346800425598596\n",
      "Gradient Descent(861/999): loss=0.4834676915187515\n",
      "Gradient Descent(862/999): loss=0.48346737878175483\n",
      "Gradient Descent(863/999): loss=0.4834670660449961\n",
      "Gradient Descent(864/999): loss=0.4834667533084754\n",
      "Gradient Descent(865/999): loss=0.4834664405721926\n",
      "Gradient Descent(866/999): loss=0.48346612783614773\n",
      "Gradient Descent(867/999): loss=0.48346581510034087\n",
      "Gradient Descent(868/999): loss=0.48346550236477176\n",
      "Gradient Descent(869/999): loss=0.48346518962944085\n",
      "Gradient Descent(870/999): loss=0.4834648768943477\n",
      "Gradient Descent(871/999): loss=0.4834645641594925\n",
      "Gradient Descent(872/999): loss=0.48346425142487515\n",
      "Gradient Descent(873/999): loss=0.48346393869049586\n",
      "Gradient Descent(874/999): loss=0.48346362595635456\n",
      "Gradient Descent(875/999): loss=0.48346331322245095\n",
      "Gradient Descent(876/999): loss=0.4834630004887855\n",
      "Gradient Descent(877/999): loss=0.4834626877553579\n",
      "Gradient Descent(878/999): loss=0.48346237502216827\n",
      "Gradient Descent(879/999): loss=0.48346206228921657\n",
      "Gradient Descent(880/999): loss=0.4834617495565028\n",
      "Gradient Descent(881/999): loss=0.48346143682402687\n",
      "Gradient Descent(882/999): loss=0.48346112409178893\n",
      "Gradient Descent(883/999): loss=0.4834608113597889\n",
      "Gradient Descent(884/999): loss=0.4834604986280268\n",
      "Gradient Descent(885/999): loss=0.48346018589650286\n",
      "Gradient Descent(886/999): loss=0.4834598731652166\n",
      "Gradient Descent(887/999): loss=0.48345956043416827\n",
      "Gradient Descent(888/999): loss=0.48345924770335796\n",
      "Gradient Descent(889/999): loss=0.48345893497278547\n",
      "Gradient Descent(890/999): loss=0.483458622242451\n",
      "Gradient Descent(891/999): loss=0.48345830951235447\n",
      "Gradient Descent(892/999): loss=0.48345799678249585\n",
      "Gradient Descent(893/999): loss=0.48345768405287515\n",
      "Gradient Descent(894/999): loss=0.4834573713234924\n",
      "Gradient Descent(895/999): loss=0.4834570585943477\n",
      "Gradient Descent(896/999): loss=0.48345674586544063\n",
      "Gradient Descent(897/999): loss=0.48345643313677167\n",
      "Gradient Descent(898/999): loss=0.4834561204083407\n",
      "Gradient Descent(899/999): loss=0.4834558076801475\n",
      "Gradient Descent(900/999): loss=0.48345549495219237\n",
      "Gradient Descent(901/999): loss=0.4834551822244751\n",
      "Gradient Descent(902/999): loss=0.4834548694969958\n",
      "Gradient Descent(903/999): loss=0.48345455676975435\n",
      "Gradient Descent(904/999): loss=0.48345424404275084\n",
      "Gradient Descent(905/999): loss=0.4834539313159853\n",
      "Gradient Descent(906/999): loss=0.4834536185894578\n",
      "Gradient Descent(907/999): loss=0.48345330586316804\n",
      "Gradient Descent(908/999): loss=0.4834529931371162\n",
      "Gradient Descent(909/999): loss=0.4834526804113024\n",
      "Gradient Descent(910/999): loss=0.48345236768572647\n",
      "Gradient Descent(911/999): loss=0.4834520549603885\n",
      "Gradient Descent(912/999): loss=0.4834517422352885\n",
      "Gradient Descent(913/999): loss=0.48345142951042636\n",
      "Gradient Descent(914/999): loss=0.48345111678580205\n",
      "Gradient Descent(915/999): loss=0.4834508040614159\n",
      "Gradient Descent(916/999): loss=0.4834504913372674\n",
      "Gradient Descent(917/999): loss=0.483450178613357\n",
      "Gradient Descent(918/999): loss=0.4834498658896846\n",
      "Gradient Descent(919/999): loss=0.48344955316625\n",
      "Gradient Descent(920/999): loss=0.48344924044305326\n",
      "Gradient Descent(921/999): loss=0.4834489277200946\n",
      "Gradient Descent(922/999): loss=0.4834486149973738\n",
      "Gradient Descent(923/999): loss=0.48344830227489094\n",
      "Gradient Descent(924/999): loss=0.48344798955264584\n",
      "Gradient Descent(925/999): loss=0.48344767683063894\n",
      "Gradient Descent(926/999): loss=0.48344736410886974\n",
      "Gradient Descent(927/999): loss=0.4834470513873385\n",
      "Gradient Descent(928/999): loss=0.4834467386660454\n",
      "Gradient Descent(929/999): loss=0.48344642594499\n",
      "Gradient Descent(930/999): loss=0.4834461132241726\n",
      "Gradient Descent(931/999): loss=0.4834458005035931\n",
      "Gradient Descent(932/999): loss=0.4834454877832515\n",
      "Gradient Descent(933/999): loss=0.4834451750631479\n",
      "Gradient Descent(934/999): loss=0.48344486234328216\n",
      "Gradient Descent(935/999): loss=0.48344454962365435\n",
      "Gradient Descent(936/999): loss=0.48344423690426447\n",
      "Gradient Descent(937/999): loss=0.4834439241851125\n",
      "Gradient Descent(938/999): loss=0.48344361146619846\n",
      "Gradient Descent(939/999): loss=0.4834432987475224\n",
      "Gradient Descent(940/999): loss=0.4834429860290842\n",
      "Gradient Descent(941/999): loss=0.483442673310884\n",
      "Gradient Descent(942/999): loss=0.4834423605929216\n",
      "Gradient Descent(943/999): loss=0.483442047875197\n",
      "Gradient Descent(944/999): loss=0.48344173515771055\n",
      "Gradient Descent(945/999): loss=0.48344142244046195\n",
      "Gradient Descent(946/999): loss=0.48344110972345133\n",
      "Gradient Descent(947/999): loss=0.48344079700667864\n",
      "Gradient Descent(948/999): loss=0.48344048429014375\n",
      "Gradient Descent(949/999): loss=0.4834401715738468\n",
      "Gradient Descent(950/999): loss=0.48343985885778784\n",
      "Gradient Descent(951/999): loss=0.4834395461419668\n",
      "Gradient Descent(952/999): loss=0.4834392334263836\n",
      "Gradient Descent(953/999): loss=0.4834389207110384\n",
      "Gradient Descent(954/999): loss=0.483438607995931\n",
      "Gradient Descent(955/999): loss=0.48343829528106164\n",
      "Gradient Descent(956/999): loss=0.4834379825664302\n",
      "Gradient Descent(957/999): loss=0.4834376698520367\n",
      "Gradient Descent(958/999): loss=0.483437357137881\n",
      "Gradient Descent(959/999): loss=0.4834370444239633\n",
      "Gradient Descent(960/999): loss=0.4834367317102835\n",
      "Gradient Descent(961/999): loss=0.48343641899684164\n",
      "Gradient Descent(962/999): loss=0.48343610628363765\n",
      "Gradient Descent(963/999): loss=0.48343579357067157\n",
      "Gradient Descent(964/999): loss=0.48343548085794347\n",
      "Gradient Descent(965/999): loss=0.4834351681454532\n",
      "Gradient Descent(966/999): loss=0.4834348554332009\n",
      "Gradient Descent(967/999): loss=0.4834345427211865\n",
      "Gradient Descent(968/999): loss=0.48343423000941005\n",
      "Gradient Descent(969/999): loss=0.48343391729787144\n",
      "Gradient Descent(970/999): loss=0.4834336045865708\n",
      "Gradient Descent(971/999): loss=0.48343329187550815\n",
      "Gradient Descent(972/999): loss=0.48343297916468325\n",
      "Gradient Descent(973/999): loss=0.4834326664540964\n",
      "Gradient Descent(974/999): loss=0.48343235374374743\n",
      "Gradient Descent(975/999): loss=0.4834320410336364\n",
      "Gradient Descent(976/999): loss=0.48343172832376324\n",
      "Gradient Descent(977/999): loss=0.48343141561412795\n",
      "Gradient Descent(978/999): loss=0.4834311029047307\n",
      "Gradient Descent(979/999): loss=0.48343079019557117\n",
      "Gradient Descent(980/999): loss=0.48343047748664975\n",
      "Gradient Descent(981/999): loss=0.4834301647779662\n",
      "Gradient Descent(982/999): loss=0.48342985206952055\n",
      "Gradient Descent(983/999): loss=0.4834295393613128\n",
      "Gradient Descent(984/999): loss=0.4834292266533429\n",
      "Gradient Descent(985/999): loss=0.483428913945611\n",
      "Gradient Descent(986/999): loss=0.483428601238117\n",
      "Gradient Descent(987/999): loss=0.48342828853086095\n",
      "Gradient Descent(988/999): loss=0.4834279758238427\n",
      "Gradient Descent(989/999): loss=0.48342766311706237\n",
      "Gradient Descent(990/999): loss=0.4834273504105201\n",
      "Gradient Descent(991/999): loss=0.4834270377042157\n",
      "Gradient Descent(992/999): loss=0.48342672499814915\n",
      "Gradient Descent(993/999): loss=0.4834264122923205\n",
      "Gradient Descent(994/999): loss=0.4834260995867297\n",
      "Gradient Descent(995/999): loss=0.483425786881377\n",
      "Gradient Descent(996/999): loss=0.4834254741762621\n",
      "Gradient Descent(997/999): loss=0.48342516147138515\n",
      "Gradient Descent(998/999): loss=0.4834248487667461\n",
      "Gradient Descent(999/999): loss=0.4834245360623449\n",
      "Gradient Descent: execution time=0.449 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "weights, loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07241289396950215\n",
      "[-4.64988198  0.06344158]\n",
      "Least Squares, rmse=0.07241289396950215\n"
     ]
    }
   ],
   "source": [
    "weights, loss = least_squares(y, tx)\n",
    "\n",
    "print(loss)\n",
    "print(weights)\n",
    "\n",
    "rmse = compute_loss(y, tx, weights)\n",
    "print(\"Least Squares, rmse={loss}\".format(loss=rmse)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.699\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.701\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.700\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.699\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.698\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.697\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.696\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.694\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.692\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.689\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.685\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.680\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.671\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.659\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.641\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.616\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.583\n",
      "proportion=0.9, degree=10, lambda=0.000, Training RMSE=0.375, Testing RMSE=0.541\n",
      "proportion=0.9, degree=10, lambda=0.001, Training RMSE=0.375, Testing RMSE=0.490\n",
      "proportion=0.9, degree=10, lambda=0.001, Training RMSE=0.375, Testing RMSE=0.435\n",
      "proportion=0.9, degree=10, lambda=0.001, Training RMSE=0.375, Testing RMSE=0.391\n",
      "proportion=0.9, degree=10, lambda=0.002, Training RMSE=0.375, Testing RMSE=0.378\n",
      "proportion=0.9, degree=10, lambda=0.004, Training RMSE=0.375, Testing RMSE=0.395\n",
      "proportion=0.9, degree=10, lambda=0.006, Training RMSE=0.376, Testing RMSE=0.407\n",
      "proportion=0.9, degree=10, lambda=0.009, Training RMSE=0.377, Testing RMSE=0.390\n",
      "proportion=0.9, degree=10, lambda=0.015, Training RMSE=0.378, Testing RMSE=0.387\n",
      "proportion=0.9, degree=10, lambda=0.023, Training RMSE=0.380, Testing RMSE=0.496\n",
      "proportion=0.9, degree=10, lambda=0.037, Training RMSE=0.382, Testing RMSE=0.709\n",
      "proportion=0.9, degree=10, lambda=0.060, Training RMSE=0.385, Testing RMSE=0.948\n",
      "proportion=0.9, degree=10, lambda=0.095, Training RMSE=0.389, Testing RMSE=1.136\n",
      "proportion=0.9, degree=10, lambda=0.153, Training RMSE=0.394, Testing RMSE=1.196\n",
      "proportion=0.9, degree=10, lambda=0.244, Training RMSE=0.399, Testing RMSE=1.024\n",
      "proportion=0.9, degree=10, lambda=0.391, Training RMSE=0.407, Testing RMSE=1.056\n",
      "proportion=0.9, degree=10, lambda=0.625, Training RMSE=0.411, Testing RMSE=0.638\n",
      "proportion=0.9, degree=10, lambda=1.000, Training RMSE=0.416, Testing RMSE=0.473\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-10, 0, 50)\n",
    "    # split data\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression with different lambda\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ridge regression\n",
    "        weight, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2 * compute_loss(y_tr, tx_tr, weight)))\n",
    "        rmse_te.append(np.sqrt(2 * compute_loss(y_te, tx_te, weight)))\n",
    "\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    \n",
    "seed = 1\n",
    "degree = 10\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 23), (250000,))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=184884.08231760474\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, tx):\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    # init parameters\n",
    "    max_iter = 3000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    # tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    t = tx.dot(w);\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        \n",
    "        # log info\n",
    "        #if iter % 100 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))       \n",
    "                \n",
    "        # converge criterion\n",
    "        # losses.append(loss)\n",
    "        # if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            # break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    print(\"loss={l}\".format(l=compute_log_loss(y, tx, w)))\n",
    "    return w\n",
    "\n",
    "w = logistic_regression_gradient_descent_demo(y, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 60 / 60\n",
      "Progress: 59 / 60\n",
      "Progress: 58 / 60\n",
      "Progress: 57 / 60\n",
      "Progress: 56 / 60\n",
      "Progress: 55 / 60\n",
      "Progress: 54 / 60\n",
      "Progress: 53 / 60\n",
      "Progress: 52 / 60\n",
      "Progress: 51 / 60\n",
      "Progress: 50 / 60\n",
      "Progress: 49 / 60\n",
      "Progress: 48 / 60\n",
      "Progress: 47 / 60\n",
      "Progress: 46 / 60\n",
      "Progress: 45 / 60\n",
      "Progress: 44 / 60\n",
      "Progress: 43 / 60\n",
      "Progress: 42 / 60\n",
      "Progress: 41 / 60\n",
      "Progress: 40 / 60\n",
      "Progress: 39 / 60\n",
      "Progress: 38 / 60\n",
      "Progress: 37 / 60\n",
      "Progress: 36 / 60\n",
      "Progress: 35 / 60\n",
      "Progress: 34 / 60\n",
      "Progress: 33 / 60\n",
      "Progress: 32 / 60\n",
      "Progress: 31 / 60\n",
      "Progress: 30 / 60\n",
      "Progress: 29 / 60\n",
      "Progress: 28 / 60\n",
      "Progress: 27 / 60\n",
      "Progress: 26 / 60\n",
      "Progress: 25 / 60\n",
      "Progress: 24 / 60\n",
      "Progress: 23 / 60\n",
      "Progress: 22 / 60\n",
      "Progress: 21 / 60\n",
      "Progress: 20 / 60\n",
      "Progress: 19 / 60\n",
      "Progress: 18 / 60\n",
      "Progress: 17 / 60\n",
      "Progress: 16 / 60\n",
      "Progress: 15 / 60\n",
      "Progress: 14 / 60\n",
      "Progress: 13 / 60\n",
      "Progress: 12 / 60\n",
      "Progress: 11 / 60\n",
      "Progress: 10 / 60\n",
      "Progress: 9 / 60\n",
      "Progress: 8 / 60\n",
      "Progress: 7 / 60\n",
      "Progress: 6 / 60\n",
      "Progress: 5 / 60\n",
      "Progress: 4 / 60\n",
      "Progress: 3 / 60\n",
      "Progress: 2 / 60\n",
      "Progress: 1 / 60\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo(y,x):\n",
    "    seed = 32\n",
    "    degree = 10\n",
    "    k_fold = 4\n",
    "    step = 15\n",
    "    lambdas = np.logspace(-10, 0, step)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    progress = step * k_fold\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ridge regression\n",
    "        for k in range(4):\n",
    "            print(\"Progress: \" + str(progress) + \" / \" + str(step * k_fold))\n",
    "            progress = progress - 1\n",
    "            [a,b] = cross_validation(y,x,k_indices,k,lambda_,degree)\n",
    "            rmse_tr.append(a)\n",
    "            rmse_te.append(b)\n",
    "    rmse_tr = np.asarray(rmse_tr).reshape(-1,4)\n",
    "    rmse_te = np.asarray(rmse_te).reshape(-1,4)\n",
    "    rmse_tr = np.mean(rmse_tr,axis=1)\n",
    "    rmse_te = np.mean(rmse_te,axis=1)\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************   \n",
    "    cross_validation_visualization(lambdas,rmse_tr,rmse_te)\n",
    "cross_validation_demo(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rmse_te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-53677ea89553>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rmse_te' is not defined"
     ]
    }
   ],
   "source": [
    "print(rmse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label,test_data,test_id_ = load_csv_data(\"test.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,mean_test,std_x_test = standardize(test_data)\n",
    "y_test,tx_test = build_model_data(test_label,x_test)\n",
    "tx_test_poly = build_poly(tx_test, degree = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erdembocugoz/Desktop/Machine Learning/MLProject1/implementations.py:130: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=184883.9378858205\n"
     ]
    }
   ],
   "source": [
    "#weight = logistic_regression_gradient_descent_demo(y,tx)\n",
    "weights = logistic_regression_gradient_descent_demo(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights,tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_id_,y_pred,\"logisticReg_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
