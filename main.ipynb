{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING PROJECT I\n",
    "\n",
    "#### TEAM MEMBERS: ASLI YORUSUN - ERDEM BOCUGOZ - SERIF SONER SERBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from test import *\n",
    "from data_cleaning import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction,data,id_ = load_csv_data(\"train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.where(data==-999, np.nan, data)\n",
    "#null_counts = data.isnull().sum()\n",
    "#print(null_counts)\n",
    "\n",
    "data = np.delete(data, [4,5,6,12,26,27,28], 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 31))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, mean_x, std_x = standardize(data)\n",
    "y, tx = build_model_data(prediction,x)\n",
    "\n",
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST LEAST SQUARES AND RIDGE REGRESSION\n",
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    data = np.loadtxt(\"dataEx3.csv\", delimiter=\",\", skiprows=1, unpack=True)\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    return x, y\n",
    "\n",
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1), (200, 3))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST LOG REG\n",
    "\n",
    "def de_standardize(x, mean_x, std_x):\n",
    "    \"\"\"Reverse the procedure of standardization.\"\"\"\n",
    "    x = x * std_x\n",
    "    x = x + mean_x\n",
    "    return x\n",
    "\n",
    "def visualization(y, x, mean_x, std_x, w, save_name):\n",
    "    \"\"\"visualize the raw data as well as the classification result.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    # plot raw data\n",
    "    x = de_standardize(x, mean_x, std_x)\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    males = np.where(y == 1)\n",
    "    females = np.where(y == 0)\n",
    "    ax1.scatter(\n",
    "        x[males, 0], x[males, 1],\n",
    "        marker='.', color=[0.06, 0.06, 1], s=20)\n",
    "    ax1.scatter(\n",
    "        x[females, 0], x[females, 1],\n",
    "        marker='*', color=[1, 0.06, 0.06], s=20)\n",
    "    ax1.set_xlabel(\"Height\")\n",
    "    ax1.set_ylabel(\"Weight\")\n",
    "    ax1.grid()\n",
    "    # plot raw data with decision boundary\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    height = np.arange(\n",
    "        np.min(x[:, 0]), np.max(x[:, 0]) + 0.01, step=0.01)\n",
    "    weight = np.arange(\n",
    "        np.min(x[:, 1]), np.max(x[:, 1]) + 1, step=1)\n",
    "    hx, hy = np.meshgrid(height, weight)\n",
    "    hxy = (np.c_[hx.reshape(-1), hy.reshape(-1)] - mean_x) / std_x\n",
    "    x_temp = np.c_[np.ones((hxy.shape[0], 1)), hxy]\n",
    "    prediction = x_temp.dot(w) > 0.5\n",
    "    prediction = prediction.reshape((weight.shape[0], height.shape[0]))\n",
    "    ax2.contourf(hx, hy, prediction, 1)\n",
    "    ax2.scatter(\n",
    "        x[males, 0], x[males, 1],\n",
    "        marker='.', color=[0.06, 0.06, 1], s=20)\n",
    "    ax2.scatter(\n",
    "        x[females, 0], x[females, 1],\n",
    "        marker='*', color=[1, 0.06, 0.06], s=20)\n",
    "    ax2.set_xlabel(\"Height\")\n",
    "    ax2.set_ylabel(\"Weight\")\n",
    "    ax2.set_xlim([min(x[:, 0]), max(x[:, 0])])\n",
    "    ax2.set_ylim([min(x[:, 1]), max(x[:, 1])])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_name)\n",
    "\n",
    "def load_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    path_dataset = \"height_weight_genders.csv\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    height = data[:, 0]\n",
    "    weight = data[:, 1]\n",
    "    gender = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "        converters={0: lambda x: 0 if b\"Male\" in x else 1})\n",
    "    # Convert to metric system\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "    return height, weight, gender\n",
    "\n",
    "\n",
    "def sample_data(y, x, seed, size_samples):\n",
    "    \"\"\"sample from dataset.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    num_observations = y.shape[0]\n",
    "    random_permuted_indices = np.random.permutation(num_observations)\n",
    "    y = y[random_permuted_indices]\n",
    "    x = x[random_permuted_indices]\n",
    "    return y[:size_samples], x[:size_samples]\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "\n",
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "#initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "#weights, loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.00001\n",
    "\n",
    "# Initialization\n",
    "#initial_w = np.zeros(tx.shape[1])\n",
    "\n",
    "#weights, loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07241289396950215\n",
      "[-4.64988198  0.06344158]\n",
      "Least Squares, rmse=0.07241289396950215\n"
     ]
    }
   ],
   "source": [
    "weights, loss = least_squares(y, tx)\n",
    "\n",
    "print(loss)\n",
    "print(weights)\n",
    "\n",
    "rmse = compute_loss(y, tx, weights)\n",
    "print(\"Least Squares, rmse={loss}\".format(loss=rmse)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-10, 0, 50)\n",
    "    # split data\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression with different lambda\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ridge regression\n",
    "        weight, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2 * compute_loss(y_tr, tx_tr, weight)))\n",
    "        rmse_te.append(np.sqrt(2 * compute_loss(y_te, tx_te, weight)))\n",
    "\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    \n",
    "seed = 1\n",
    "degree = 10\n",
    "split_ratio = 0.9\n",
    "#ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 2), (200, 1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1)\n",
      "(250000, 31)\n",
      "Current iteration=0, loss=0.693147180559945\n",
      "False\n",
      "Current iteration=100, loss=0.6106230547030241\n",
      "False\n",
      "Current iteration=200, loss=0.5782196199392977\n",
      "False\n",
      "Current iteration=300, loss=0.5603983556493879\n",
      "False\n",
      "Current iteration=400, loss=0.5492383715980815\n",
      "False\n",
      "Current iteration=500, loss=0.5416759569568736\n",
      "False\n",
      "Current iteration=600, loss=0.5362579718214529\n",
      "False\n",
      "Current iteration=700, loss=0.5322084407445309\n",
      "False\n",
      "Current iteration=800, loss=0.5290768081249816\n",
      "False\n",
      "Current iteration=900, loss=0.5265847340360554\n",
      "False\n",
      "loss=0.5245520334491464\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, tx):\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    # init parameters\n",
    "    max_iter = 1000 # 3000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01 # 0.1\n",
    "    losses = []\n",
    "    \n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    print(y.shape)\n",
    "    print(tx.shape)\n",
    "\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)     \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))       \n",
    "            pred = sigmoid(tx.dot(w))\n",
    "            print (np.any(pred == 0)) \n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    # visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    print(\"loss={l}\".format(l=compute_log_loss(y, tx, w)))\n",
    "    return w\n",
    "\n",
    "w = logistic_regression_gradient_descent_demo(y, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo(y,x):\n",
    "    seed = 32\n",
    "    degree = 10\n",
    "    k_fold = 4\n",
    "    step = 15\n",
    "    lambdas = np.logspace(-10, 0, step)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    progress = step * k_fold\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ridge regression\n",
    "        for k in range(4):\n",
    "            print(\"Progress: \" + str(progress) + \" / \" + str(step * k_fold))\n",
    "            progress = progress - 1\n",
    "            [a,b] = cross_validation(y,x,k_indices,k,lambda_,degree)\n",
    "            rmse_tr.append(a)\n",
    "            rmse_te.append(b)\n",
    "    rmse_tr = np.asarray(rmse_tr).reshape(-1,4)\n",
    "    rmse_te = np.asarray(rmse_te).reshape(-1,4)\n",
    "    rmse_tr = np.mean(rmse_tr,axis=1)\n",
    "    rmse_te = np.mean(rmse_te,axis=1)\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************   \n",
    "    cross_validation_visualization(lambdas,rmse_tr,rmse_te)\n",
    "#cross_validation_demo(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rmse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label,test_data,test_id_ = load_csv_data(\"test.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,mean_test,std_x_test = standardize(test_data)\n",
    "y_test,tx_test = build_model_data(test_label,x_test)\n",
    "tx_test_poly = build_poly(tx_test, degree = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erdembocugoz/Desktop/Machine Learning/MLProject1/implementations.py:130: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=184883.9378858205\n"
     ]
    }
   ],
   "source": [
    "#weight = logistic_regression_gradient_descent_demo(y,tx)\n",
    "weights = logistic_regression_gradient_descent_demo(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights,tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(test_id_,y_pred,\"logisticReg_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
